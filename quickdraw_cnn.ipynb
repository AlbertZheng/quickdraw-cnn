{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickdraw-cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/quickdraw-cnn/blob/master/quickdraw_cnn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "qs0seV43MwNv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## \"You draw, I guess.\" is a POC project that uses CNN to recognize sketch drawings.\n",
        "### The CNN was trained to recognize 345 classes using <a href='https://github.com/googlecreativelab/quickdraw-dataset'>\"The Quick, Draw! Dataset\" </a> of Google awesome \"猜画小歌\" Wechat App."
      ]
    },
    {
      "metadata": {
        "id": "0_FT6bGsOY22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install dependent packages"
      ]
    },
    {
      "metadata": {
        "id": "EDJoFej9MZ83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2298
        },
        "outputId": "0a82a0bb-d15f-4f32-8cb3-3d3c5c7cc783"
      },
      "cell_type": "code",
      "source": [
        "!pip install \"tensorlayer>=1.10\"\n",
        "!pip install tensorflowjs\n",
        "!pip list|grep tensor"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorlayer>=1.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/e2/736458723564163cfd87e3a9719a9fdece9011429bf556fb910d3691352e/tensorlayer-1.10.1-py2.py3-none-any.whl (313kB)\n",
            "\u001b[K    100% |████████████████████████████████| 317kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (1.14.6)\n",
            "Collecting progressbar2<3.39,>=3.38 (from tensorlayer>=1.10)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
            "Collecting imageio<2.5,>=2.3 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b4/cbb592964dfd71a9de6a5b08f882fd334fb99ae09ddc82081dbb2f718c81/imageio-2.4.1.tar.gz (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 937kB/s \n",
            "\u001b[?25hCollecting scipy<1.2,>=1.1 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 31.2MB 1.3MB/s \n",
            "\u001b[?25hCollecting tqdm<4.26,>=4.23 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/e0/52b2faaef4fd87f86eb8a8f1afa2cd6eb11146822033e29c04ac48ada32c/tqdm-4.25.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (1.10.11)\n",
            "Requirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (0.19.2)\n",
            "Collecting requests<2.20,>=2.19 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 23.6MB/s \n",
            "\u001b[?25hCollecting scikit-image<0.15,>=0.14 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/79/cefff573a53ca3fb4c390739d19541b95f371e24d2990aed4cd8837971f0/scikit_image-0.14.0-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 25.3MB 2.1MB/s \n",
            "\u001b[?25hCollecting lxml<4.3,>=4.2 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 6.6MB/s \n",
            "\u001b[?25hCollecting matplotlib<2.3,>=2.2 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer>=1.10) (1.11.0)\n",
            "Collecting python-utils>=2.3.0 (from progressbar2<3.39,>=3.38->tensorlayer>=1.10)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<2.5,>=2.3->tensorlayer>=1.10) (4.0.0)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (2018.8.24)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (3.0.4)\n",
            "Collecting cloudpickle>=0.2.1 (from scikit-image<0.15,>=0.14->tensorlayer>=1.10)\n",
            "  Downloading https://files.pythonhosted.org/packages/98/d6/a78a4589234cc6f47f29665c1225f30467db5fdaf4ca1fb52b0685bff108/cloudpickle-0.5.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (1.0.1)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (2.2)\n",
            "Collecting dask[array]>=0.9.0 (from scikit-image<0.15,>=0.14->tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/cc/d8b279ad3512e682069263de0494eeb9907ebe6b98bef01424be36421e13/dask-0.19.2-py2.py3-none-any.whl (657kB)\n",
            "\u001b[K    100% |████████████████████████████████| 665kB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (0.10.0)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib<2.3,>=2.2->tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
            "\u001b[K    100% |████████████████████████████████| 952kB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2018.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2.2.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio<2.5,>=2.3->tensorlayer>=1.10) (0.46)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer>=1.10) (4.3.0)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer>=1.10) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer>=1.10) (39.1.0)\n",
            "Building wheels for collected packages: imageio\n",
            "  Running setup.py bdist_wheel for imageio ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e0/43/31/605de9372ceaf657f152d3d5e82f42cf265d81db8bbe63cde1\n",
            "Successfully built imageio\n",
            "\u001b[31mscikit-image 0.14.0 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: python-utils, progressbar2, imageio, scipy, tqdm, requests, cloudpickle, kiwisolver, matplotlib, dask, scikit-image, lxml, tensorlayer\n",
            "  Found existing installation: scipy 0.19.1\n",
            "    Uninstalling scipy-0.19.1:\n",
            "      Successfully uninstalled scipy-0.19.1\n",
            "  Found existing installation: tqdm 4.26.0\n",
            "    Uninstalling tqdm-4.26.0:\n",
            "      Successfully uninstalled tqdm-4.26.0\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "  Found existing installation: matplotlib 2.1.2\n",
            "    Uninstalling matplotlib-2.1.2:\n",
            "      Successfully uninstalled matplotlib-2.1.2\n",
            "  Found existing installation: scikit-image 0.13.1\n",
            "    Uninstalling scikit-image-0.13.1:\n",
            "      Successfully uninstalled scikit-image-0.13.1\n",
            "Successfully installed cloudpickle-0.5.6 dask-0.19.2 imageio-2.4.1 kiwisolver-1.0.1 lxml-4.2.5 matplotlib-2.2.3 progressbar2-3.38.0 python-utils-2.3.0 requests-2.19.1 scikit-image-0.14.0 scipy-1.1.0 tensorlayer-1.10.1 tqdm-4.25.0\n",
            "Collecting tensorflowjs\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/ba/a4372caa01427c179d271603fa305c9239ac2300d7b065ddc6fac46332f4/tensorflowjs-0.6.1-py3-none-any.whl\n",
            "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.8.0)\n",
            "Collecting keras==2.2.2 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/7d/b1dedde8af99bd82f20ed7e9697aac0597de3049b1f786aa2aac3b9bd4da/Keras-2.2.2-py2.py3-none-any.whl (299kB)\n",
            "\u001b[K    100% |████████████████████████████████| 307kB 7.9MB/s \n",
            "\u001b[?25hCollecting numpy==1.15.1 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/94/7049fed8373c52839c8cde619acaf2c9b83082b935e5aa8c0fa27a4a8bcc/numpy-1.15.1-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.9MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub==0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.1.1)\n",
            "Collecting tensorflow==1.10.1 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (58.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 58.4MB 846kB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.11.0)\n",
            "Collecting keras-applications==1.0.4 (from keras==2.2.2->tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 17.7MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing==1.0.2 (from keras==2.2.2->tensorflowjs)\n",
            "  Downloading https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2->tensorflowjs) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.1.1->tensorflowjs) (3.6.1)\n",
            "Requirement already satisfied: tensorboard<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1->tensorflowjs) (1.10.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1->tensorflowjs) (0.5.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1->tensorflowjs) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1->tensorflowjs) (0.31.1)\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1->tensorflowjs) (39.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.1->tensorflowjs) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.1->tensorflowjs) (3.0.1)\n",
            "\u001b[31mtensorflow 1.10.1 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mscikit-image 0.14.0 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, keras-applications, keras-preprocessing, keras, tensorflow, tensorflowjs\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "  Found existing installation: Keras-Applications 1.0.5\n",
            "    Uninstalling Keras-Applications-1.0.5:\n",
            "      Successfully uninstalled Keras-Applications-1.0.5\n",
            "  Found existing installation: Keras-Preprocessing 1.0.3\n",
            "    Uninstalling Keras-Preprocessing-1.0.3:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.0.3\n",
            "  Found existing installation: Keras 2.1.6\n",
            "    Uninstalling Keras-2.1.6:\n",
            "      Successfully uninstalled Keras-2.1.6\n",
            "  Found existing installation: tensorflow 1.11.0rc2\n",
            "    Uninstalling tensorflow-1.11.0rc2:\n",
            "      Successfully uninstalled tensorflow-1.11.0rc2\n",
            "Successfully installed keras-2.2.2 keras-applications-1.0.4 keras-preprocessing-1.0.2 numpy-1.15.1 tensorflow-1.10.1 tensorflowjs-0.6.1\n",
            "tensorboard              1.10.0   \n",
            "tensorflow               1.10.1   \n",
            "tensorflow-hub           0.1.1    \n",
            "tensorflowjs             0.6.1    \n",
            "tensorlayer              1.10.1   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QV6AypOzOiws",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependences, and check if GPU is available"
      ]
    },
    {
      "metadata": {
        "id": "W4-1-Q-EOi8j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "from random import randint\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import *\n",
        "from tensorflow.python import debug as tfdebug\n",
        "\"\"\"\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('### device name: {} ###'.format(device_name))\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('*** GPU device not found ***')\n",
        "print('### Found GPU at: {} ###'.format(device_name))\n",
        "\"\"\"\n",
        "\"\"\" Notice to put ```import matplotlib.pyplot``` after imports of tensorlayer, \n",
        "otherwise you will get below warning:\n",
        "\n",
        "This call to matplotlib.use() has no effect because the backend has already\n",
        "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
        "or matplotlib.backends is imported for the first time.\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q27izcVnPcQu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the Quick, Draw dataset"
      ]
    },
    {
      "metadata": {
        "id": "QeDBh_7WPc0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "62fb564d-93f7-45cb-9223-455955c9869e"
      },
      "cell_type": "code",
      "source": [
        "working_directory = 'data'\n",
        "dataset_directory = 'data/quickdraw'\n",
        "categories_filename = 'categories.txt'\n",
        "categories_file_url_source = 'https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/'\n",
        "npy_dataset_url_source = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "\n",
        "X = \"X\"\n",
        "y = \"y\"\n",
        "\n",
        "X_NUMPY_DTYPE = np.float32\n",
        "y_NUMPY_DTYPE = np.int64\n",
        "X_TF_DTYPE = tf.float32\n",
        "y_TF_DTYPE = tf.int32\n",
        "\n",
        "image_height = 28\n",
        "image_width = 28\n",
        "image_depth = 1\n",
        "image_size = image_height * image_width * image_depth\n",
        "input_layer_X_shape = [image_height, image_width, image_depth]\n",
        "input_layer_X_shape_batch = [-1, image_height, image_width, image_depth]\n",
        "\n",
        "n_category = 10  # The maximum of category number is up to 345\n",
        "n_train_example_per_category = 20000\n",
        "\n",
        "\n",
        "def print_dataset_shape(X_name, X, y_name, y):\n",
        "    print(X_name + '.shape ', X.shape, end='\\t,\\t')\n",
        "    print(y_name + '.shape ', y.shape)\n",
        "    print('%s.dtype %s\\t,\\t%s.dtype %s' % (X_name, X.dtype, y_name, y.dtype))\n",
        "\n",
        "\n",
        "def show_image(X, y, categories):\n",
        "    plt.imshow(X.reshape(image_height, image_width), cmap=\"gray\", interpolation='nearest')\n",
        "    plt.title(categories[y])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_quickdraw_dataset(\n",
        "    n_category=10, n_train_example_per_category=20000, plotable=False\n",
        "):\n",
        "    \"\"\" Download the quick draw dataset. \"\"\"\n",
        "    n_validation_example_per_category = int(n_train_example_per_category / 0.7 * 0.2)\n",
        "    n_test_example_per_category = int(n_train_example_per_category / 0.7 * 0.1)\n",
        "\n",
        "    tl.logging.info(\"Downloading the quick draw dataset > {}\".format(dataset_directory))\n",
        "\n",
        "    # Download the categories file\n",
        "    tl.files.utils.maybe_download_and_extract(categories_filename, dataset_directory, categories_file_url_source)\n",
        "\n",
        "    categories = [line.rstrip('\\n') for line in open(f\"{dataset_directory}/{categories_filename}\")]\n",
        "    for category_index, category_name in enumerate(categories):\n",
        "        if category_index == n_category:\n",
        "            break\n",
        "\n",
        "        filename = urllib.parse.quote(category_name) + '.npy'\n",
        "        tl.files.utils.maybe_download_and_extract(filename, dataset_directory, npy_dataset_url_source)\n",
        "\n",
        "    tl.logging.info(\"Loading the quick draw dataset into memory\")\n",
        "\n",
        "    train_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "    validation_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "    test_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "\n",
        "    for category_index, category_name in enumerate(categories):\n",
        "        if category_index == n_category:\n",
        "            break\n",
        "\n",
        "        filename = urllib.parse.quote(category_name) + '.npy'\n",
        "        data = np.load(os.path.join(dataset_directory, filename))\n",
        "        size_per_category = data.shape[0]\n",
        "        labels = np.full(size_per_category, category_index)\n",
        "\n",
        "        print(f\"### Category '{category_name}' dataset info ###\")\n",
        "        print_dataset_shape(\"data\", data, \"labels\", labels)\n",
        "\n",
        "        # Randomize the dataset\n",
        "        permutation = np.random.permutation(size_per_category)\n",
        "        data = data[permutation, :]\n",
        "        labels = labels[permutation]\n",
        "\n",
        "        number_begin = 0\n",
        "        number_end = n_train_example_per_category\n",
        "        train_set[X] = np.vstack((train_set[X], data[number_begin: number_end, :]))\n",
        "        train_set[y] = np.append(train_set[y], labels[number_begin: number_end])\n",
        "\n",
        "        number_begin += n_train_example_per_category\n",
        "        number_end += n_validation_example_per_category\n",
        "        validation_set[X] = np.vstack((validation_set[X], data[number_begin:number_end, :]))\n",
        "        validation_set[y] = np.append(validation_set[y], labels[number_begin:number_end])\n",
        "\n",
        "        number_begin += n_validation_example_per_category\n",
        "        number_end += n_test_example_per_category\n",
        "        test_set[X] = np.vstack((test_set[X], data[number_begin:number_end, :]))\n",
        "        test_set[y] = np.append(test_set[y], labels[number_begin:number_end])\n",
        "\n",
        "        print_dataset_shape(\"train_set[X]\", train_set[X], \"train_set[y]\", train_set[y])\n",
        "        print_dataset_shape(\"validation_set[X]\", validation_set[X], \"validation_set[y]\", validation_set[y])\n",
        "        print_dataset_shape(\"test_set[X]\", test_set[X], \"test_set[y]\", test_set[y])\n",
        "\n",
        "    # Reshape for CNN input\n",
        "    train_set[X] = train_set[X].reshape(input_layer_X_shape_batch)\n",
        "    validation_set[X] = validation_set[X].reshape(input_layer_X_shape_batch)\n",
        "    test_set[X] = test_set[X].reshape(input_layer_X_shape_batch)\n",
        "\n",
        "    \"\"\"\n",
        "    if plotable:\n",
        "        r = randint(0, n_category * n_train_example_per_category - 1)\n",
        "        show_image(train_set[X][r], train_set[y][r], categories)\n",
        "    \n",
        "        r = randint(0, n_category * n_validation_example_per_category - 1)\n",
        "        show_image(validation_set[X][r], validation_set[y][r], categories)\n",
        "    \n",
        "        r = randint(0, n_category * n_test_example_per_category - 1)\n",
        "        show_image(test_set[X][r], test_set[y][r], categories)\n",
        "    \"\"\"\n",
        "\n",
        "    return categories, train_set, validation_set, test_set\n",
        "\n",
        "\n",
        "# Open TensorBoard logs writer\n",
        "tfboard_file_writer = tf.summary.FileWriter('logs')\n",
        "\n",
        "# Download data\n",
        "categories, train_set, validation_set, test_set = load_quickdraw_dataset(n_category, n_train_example_per_category)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TL] Downloading the quick draw dataset > data/quickdraw\n",
            "[TL] Downloading categories.txt...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (1 of 1) |##########################| Elapsed Time: 0:00:00 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded categories.txt 2791 bytes.\n",
            "[TL] Downloading aircraft%20carrier.npy...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (11150 of 11150) |##################| Elapsed Time: 0:00:27 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded aircraft%20carrier.npy 91339216 bytes.\n",
            "[TL] Downloading airplane.npy...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (14511 of 14511) |##################| Elapsed Time: 0:00:36 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded airplane.npy 118872512 bytes.\n",
            "[TL] Downloading alarm%20clock.npy...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (11810 of 11810) |##################| Elapsed Time: 0:00:27 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded alarm%20clock.npy 96744896 bytes.\n",
            "[TL] Downloading ambulance.npy...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (14165 of 14165) |##################| Elapsed Time: 0:00:29 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded ambulance.npy 116035216 bytes.\n",
            "[TL] Downloading angel.npy...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (14331 of 14331) |##################| Elapsed Time: 0:00:31 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[TL] Succesfully downloaded angel.npy 117393104 bytes.\n",
            "[TL] Downloading animal%20migration.npy...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 21% (2802 of 13193) |####               | Elapsed Time: 0:00:06 ETA:   0:00:24"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LejFWzHKP2Dg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Function: Network model definition"
      ]
    },
    {
      "metadata": {
        "id": "d5TwN6-sP1eQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_batch_normalization(X_batch, y_batch, output_units, reuse, is_train):\n",
        "    \"\"\" Define the network model \"\"\"\n",
        "    W_init1 = tf.truncated_normal_initializer(stddev=5e-2)\n",
        "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
        "    bias_init = tf.constant_initializer(value=0.1)\n",
        "\n",
        "    with tf.variable_scope(\"model\", reuse=reuse):\n",
        "        net = InputLayer(X_batch, name='input')\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME',\n",
        "                     W_init=W_init1, b_init=None, name='cnn1')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch1')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
        "\n",
        "        net = Conv2d(net, 64, (5, 5), (1, 1), padding='SAME',\n",
        "                     W_init=W_init1, b_init=None, name='cnn2')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch2')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
        "\n",
        "        net = FlattenLayer(net, name='flatten')\n",
        "        net = DenseLayer(net, 384, act=tf.nn.relu,\n",
        "                         W_init=W_init2, b_init=bias_init, name='d1relu')\n",
        "        net = DenseLayer(net, 192, act=tf.nn.relu,\n",
        "                         W_init=W_init2, b_init=bias_init, name='d2relu')\n",
        "        # The softmax() is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
        "        # speed up computation, so we use identity here.\n",
        "        # see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
        "        net = DenseLayer(net, n_units=output_units, act=None,\n",
        "                         W_init=W_init2, name='output')\n",
        "        y_prediction_batch_without_softmax = net.outputs\n",
        "\n",
        "        # For inference by using this model\n",
        "        # y_output = tf.argmax(tf.nn.softmax(y_prediction_batch_without_softmax), 1)\n",
        "        y_output = tf.nn.softmax(y_prediction_batch_without_softmax, name=\"y_output\")\n",
        "\n",
        "        ce = tl.cost.cross_entropy(y_prediction_batch_without_softmax, y_batch, name='cost')\n",
        "\n",
        "        \"\"\" 需给后面的全连接层引入L2 normalization，惩罚模型的复杂度，避免overfitting \"\"\"\n",
        "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
        "        L2 = 0\n",
        "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
        "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
        "        # 加上L2模型复杂度惩罚项后，得到最终真正的cost\n",
        "        cost = ce + L2\n",
        "\n",
        "        correct_prediction = tf.equal(tf.cast(tf.argmax(y_prediction_batch_without_softmax, 1), y_TF_DTYPE), y_batch)\n",
        "        # correct_prediction = tf.Print(correct_prediction, [correct_prediction], \"correct_prediction: \")\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        return net, cost, accuracy, y_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jpISk8SQqCO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Train, validate and test"
      ]
    },
    {
      "metadata": {
        "id": "3iOAaIdbQoQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3890
        },
        "outputId": "7053b80b-52a1-418e-ce08-6b7fb6bd076b"
      },
      "cell_type": "code",
      "source": [
        "# Train, validate and test\n",
        "batch_size = 128\n",
        "n_epoch = 16\n",
        "n_step_per_epoch = int(len(train_set[y]) / batch_size)\n",
        "n_step = n_epoch * n_step_per_epoch\n",
        "print_freq = 1\n",
        "checkpoint_freq = 4\n",
        "learning_rate = 0.0001\n",
        "\n",
        "model_ckpt_file_name = os.path.join(working_directory, \"checkpoint\", \"model-quickdraw-cnn.ckpt\")\n",
        "resume = True  # load model, resume from previous checkpoint?\n",
        "\n",
        "\n",
        "def distort_fn(X, is_train=False):\n",
        "    # print('begin', X.shape, np.min(X), np.max(X))\n",
        "\n",
        "    if is_train == True:\n",
        "        # 1. Randomly flip the image horizontally.\n",
        "        X = tf.image.random_flip_left_right(X)\n",
        "\n",
        "    # X = tf.image.per_image_standardization(X)\n",
        "\n",
        "    # print('after norm', X.shape, np.min(X), np.max(X), np.mean(X))\n",
        "    return X\n",
        "\n",
        "\n",
        "def save_model():\n",
        "    model_type = \"saved-model\"\n",
        "    latest_model_directory = f'{model_type}-{time.strftime(\"%Y%m%d%H%M%S\", time.localtime())}'\n",
        "    saved_model_directory = os.path.join(working_directory, latest_model_directory)\n",
        "    tf.saved_model.simple_save(session, saved_model_directory,\n",
        "                               inputs={\"X\": X_batch_ph},\n",
        "                               outputs={\"y_output\": y_prediction_})\n",
        "    dist_directory = os.path.join(\".\", model_type)\n",
        "    if os.path.exists(dist_directory):\n",
        "        os.remove(dist_directory)\n",
        "    os.symlink(saved_model_directory, dist_directory, target_is_directory=True)\n",
        "\n",
        "    \n",
        "with tf.device('/cpu:0'):\n",
        "    session = tf.Session(config=config)\n",
        "\n",
        "    #\n",
        "    # Connect to tfdbg dashboard by ```http://localhost:6006#debugger```\n",
        "    # when the following command is issued.\n",
        "    #\n",
        "    # ```bash\n",
        "    # $ tensorboard --logdir logs --port 6006 --debugger_port 6064\n",
        "    # ```\n",
        "    #\n",
        "    # session = tfdebug.TensorBoardDebugWrapperSession(session, \"albert-mbp.local:6064\")\n",
        "\n",
        "    X_batch_ph = tf.placeholder(dtype=X_TF_DTYPE, shape=[None, image_height, image_width, image_depth], name='X_batch')\n",
        "    y_batch_ph = tf.placeholder(dtype=y_TF_DTYPE, shape=[None], name='y_batch')\n",
        "    # X_batch_ph = tf.placeholder(dtype=X_TF_DTYPE, shape=[batch_size, image_height, image_width, image_depth], name='X')\n",
        "    # y_batch_ph = tf.placeholder(dtype=y_TF_DTYPE, shape=[batch_size], name='y')\n",
        "\n",
        "    def perform_minibatch(run_list, X, y, batch_size, is_train=False):\n",
        "        n_batch, sum_loss, sum_accuracy = 0, 0, 0\n",
        "        for X_batch_a, y_batch_a in tl.iterate.minibatches(X, y, batch_size, shuffle=is_train):\n",
        "            # data augmentation for training\n",
        "            # X_batch_a = tl.prepro.threading_data(X_batch_a, fn=distort_fn, is_train=is_train)\n",
        "\n",
        "            cost, accuracy = 0, 0\n",
        "            if is_train:\n",
        "                _, cost, accuracy = session.run(\n",
        "                    run_list, feed_dict={X_batch_ph: X_batch_a, y_batch_ph: y_batch_a}\n",
        "                )\n",
        "            else:\n",
        "                cost, accuracy = session.run(\n",
        "                    run_list, feed_dict={X_batch_ph: X_batch_a, y_batch_ph: y_batch_a}\n",
        "                )\n",
        "\n",
        "            sum_loss += cost\n",
        "            sum_accuracy += accuracy\n",
        "            n_batch += 1\n",
        "        return n_batch, sum_loss, sum_accuracy\n",
        "\n",
        "\n",
        "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
        "        # Build the model\n",
        "        print(\"### Train Network model ###\")\n",
        "        network_, cost_, accuracy_, y_prediction_ = model_batch_normalization(\n",
        "            X_batch_ph, y_batch_ph, n_category, reuse=None, is_train=True\n",
        "        )\n",
        "        print(\"### Reuse this Train Network model for validation and test ###\")\n",
        "        _, cost_test_, accuracy_test_, y_prediction_test_ = model_batch_normalization(\n",
        "            X_batch_ph, y_batch_ph, n_category, reuse=True, is_train=False\n",
        "        )\n",
        "\n",
        "    # Define the training optimizer\n",
        "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
        "        train_op_ = tf.train.AdamOptimizer(learning_rate).minimize(cost_)\n",
        "\n",
        "    tl.layers.initialize_global_variables(session)\n",
        "\n",
        "    # Attach the graph for TensorBoard writer\n",
        "    # tfboard_file_writer.add_graph(tf.get_default_graph())\n",
        "    tfboard_file_writer.add_graph(session.graph)\n",
        "\n",
        "    if resume and os.path.isfile(model_ckpt_file_name):\n",
        "        print(\"Load existing model \" + \"!\" * 10)\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(session, model_ckpt_file_name)\n",
        "\n",
        "    print(\"### Train Network parameters ###\")\n",
        "    network_.print_params(False)\n",
        "    print(\"### Train Network layers ###\")\n",
        "    network_.print_layers()\n",
        "\n",
        "    print('   learning_rate: %f' % learning_rate)\n",
        "    print('   batch_size: %d' % batch_size)\n",
        "    print('   n_epoch: %d, step in an epoch: %d, total n_step: %d' % (n_epoch, n_step_per_epoch, n_step))\n",
        "\n",
        "    step, sum_batch, sum_loss, sum_accuracy = 0, 0, 0, 0\n",
        "    for epoch in range(n_epoch):\n",
        "        start_time = time.time()\n",
        "\n",
        "        n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "            [train_op_, cost_, accuracy_],\n",
        "            train_set[X], train_set[y], batch_size, is_train=True\n",
        "        )\n",
        "        sum_batch += n_batch_a_epoch\n",
        "        sum_loss += cost_a_epoch\n",
        "        sum_accuracy += accuracy_a_epoch\n",
        "        step += n_batch_a_epoch\n",
        "\n",
        "        assert n_batch_a_epoch == n_step_per_epoch\n",
        "\n",
        "        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
        "            print(\"Epoch %d : Step %d-%d of %d took %fs\" %\n",
        "                  (epoch, step - n_step_per_epoch, step, n_step, time.time() - start_time))\n",
        "            print(\"   train loss: %f\" % (sum_loss / sum_batch))\n",
        "            print(\"   train accuracy: %f\" % (sum_accuracy / sum_batch))\n",
        "            sum_batch, sum_loss, sum_accuracy = 0, 0, 0\n",
        "\n",
        "            n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "                [cost_test_, accuracy_test_],\n",
        "                validation_set[X], validation_set[y], batch_size\n",
        "            )\n",
        "            print(\"   validation loss: %f\" % (cost_a_epoch / n_batch_a_epoch))\n",
        "            print(\"   validation accuracy: %f\" % (accuracy_a_epoch / n_batch_a_epoch))\n",
        "\n",
        "            n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "                [cost_test_, accuracy_test_],\n",
        "                test_set[X], test_set[y], batch_size\n",
        "            )\n",
        "            print(\"   test loss: %f\" % (cost_a_epoch / n_batch_a_epoch))\n",
        "            print(\"   test accuracy: %f\" % (accuracy_a_epoch / n_batch_a_epoch))\n",
        "\n",
        "        # Save model when checkpoint\n",
        "        if (epoch + 1) % checkpoint_freq == 0:\n",
        "            print(\"Saving checkpoint... \" + \"!\" * 10)\n",
        "            saver = tf.train.Saver()\n",
        "            save_path = saver.save(session, model_ckpt_file_name)\n",
        "            print(\"Saving model... \" + \"!\" * 10)\n",
        "            save_model()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Train Network model ###\n",
            "[TL] InputLayer  model/input: (?, 28, 28, 1)\n",
            "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 3136\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (3136, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "### Reuse this Train Network model for validation and test ###\n",
            "[TL] InputLayer  model/input: (?, 28, 28, 1)\n",
            "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 3136\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (3136, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "[TL] WARNING: Function: `tensorlayer.layers.utils.initialize_global_variables` (in file: /usr/local/lib/python3.6/dist-packages/tensorlayer/layers/utils.py) is deprecated and will be removed after 2018-09-30.\n",
            "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
            "\n",
            "### Train Network parameters ###\n",
            "[TL]   param   0: model/cnn1/kernel:0  (5, 5, 1, 64)      float32_ref\n",
            "[TL]   param   1: model/batch1/beta:0  (64,)              float32_ref\n",
            "[TL]   param   2: model/batch1/gamma:0 (64,)              float32_ref\n",
            "[TL]   param   3: model/batch1/moving_mean:0 (64,)              float32_ref\n",
            "[TL]   param   4: model/batch1/moving_variance:0 (64,)              float32_ref\n",
            "[TL]   param   5: model/cnn2/kernel:0  (5, 5, 64, 64)     float32_ref\n",
            "[TL]   param   6: model/batch2/beta:0  (64,)              float32_ref\n",
            "[TL]   param   7: model/batch2/gamma:0 (64,)              float32_ref\n",
            "[TL]   param   8: model/batch2/moving_mean:0 (64,)              float32_ref\n",
            "[TL]   param   9: model/batch2/moving_variance:0 (64,)              float32_ref\n",
            "[TL]   param  10: model/d1relu/W:0     (3136, 384)        float32_ref\n",
            "[TL]   param  11: model/d1relu/b:0     (384,)             float32_ref\n",
            "[TL]   param  12: model/d2relu/W:0     (384, 192)         float32_ref\n",
            "[TL]   param  13: model/d2relu/b:0     (192,)             float32_ref\n",
            "[TL]   param  14: model/output/W:0     (192, 10)          float32_ref\n",
            "[TL]   param  15: model/output/b:0     (10,)              float32_ref\n",
            "[TL]   num of params: 1384970\n",
            "### Train Network layers ###\n",
            "[TL]   layer   0: X_batch:0            (?, 28, 28, 1)     float32\n",
            "[TL]   layer   1: model/cnn1/Conv2D:0  (?, 28, 28, 64)    float32\n",
            "[TL]   layer   2: model/batch1/Relu:0  (?, 28, 28, 64)    float32\n",
            "[TL]   layer   3: model/pool1/MaxPool:0 (?, 14, 14, 64)    float32\n",
            "[TL]   layer   4: model/cnn2/Conv2D:0  (?, 14, 14, 64)    float32\n",
            "[TL]   layer   5: model/batch2/Relu:0  (?, 14, 14, 64)    float32\n",
            "[TL]   layer   6: model/pool2/MaxPool:0 (?, 7, 7, 64)      float32\n",
            "[TL]   layer   7: model/flatten:0      (?, 3136)          float32\n",
            "[TL]   layer   8: model/d1relu/Relu:0  (?, 384)           float32\n",
            "[TL]   layer   9: model/d2relu/Relu:0  (?, 192)           float32\n",
            "[TL]   layer  10: model/output/bias_add:0 (?, 10)            float32\n",
            "   learning_rate: 0.000100\n",
            "   batch_size: 128\n",
            "   n_epoch: 16, step in an epoch: 1562, total n_step: 24992\n",
            "Epoch 0 : Step 0-1562 of 24992 took 1309.781277s\n",
            "   train loss: 3.262350\n",
            "   train accuracy: 0.785536\n",
            "   validation loss: 2.497255\n",
            "   validation accuracy: 0.852526\n",
            "   test loss: 2.490302\n",
            "   test accuracy: 0.854575\n",
            "Epoch 1 : Step 1562-3124 of 24992 took 1332.402618s\n",
            "   train loss: 2.090689\n",
            "   train accuracy: 0.872364\n",
            "   validation loss: 1.767842\n",
            "   validation accuracy: 0.880588\n",
            "   test loss: 1.762978\n",
            "   test accuracy: 0.880816\n",
            "Epoch 2 : Step 3124-4686 of 24992 took 1320.069165s\n",
            "   train loss: 1.478573\n",
            "   train accuracy: 0.890945\n",
            "   validation loss: 1.262742\n",
            "   validation accuracy: 0.890870\n",
            "   test loss: 1.256473\n",
            "   test accuracy: 0.891781\n",
            "Epoch 3 : Step 4686-6248 of 24992 took 1326.563078s\n",
            "   train loss: 1.042043\n",
            "   train accuracy: 0.901153\n",
            "   validation loss: 0.911015\n",
            "   validation accuracy: 0.896651\n",
            "   test loss: 0.908220\n",
            "   test accuracy: 0.897351\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181001151530/saved_model.pb\n",
            "Epoch 4 : Step 6248-7810 of 24992 took 1310.793887s\n",
            "   train loss: 0.751806\n",
            "   train accuracy: 0.908391\n",
            "   validation loss: 0.704498\n",
            "   validation accuracy: 0.895950\n",
            "   test loss: 0.699409\n",
            "   test accuracy: 0.897527\n",
            "Epoch 5 : Step 7810-9372 of 24992 took 1343.569342s\n",
            "   train loss: 0.575055\n",
            "   train accuracy: 0.913937\n",
            "   validation loss: 0.572353\n",
            "   validation accuracy: 0.901573\n",
            "   test loss: 0.566509\n",
            "   test accuracy: 0.902817\n",
            "Epoch 6 : Step 9372-10934 of 24992 took 1311.629247s\n",
            "   train loss: 0.471419\n",
            "   train accuracy: 0.918304\n",
            "   validation loss: 0.496682\n",
            "   validation accuracy: 0.903990\n",
            "   test loss: 0.494151\n",
            "   test accuracy: 0.904884\n",
            "Epoch 7 : Step 10934-12496 of 24992 took 1323.825355s\n",
            "   train loss: 0.407356\n",
            "   train accuracy: 0.922095\n",
            "   validation loss: 0.458254\n",
            "   validation accuracy: 0.905234\n",
            "   test loss: 0.456078\n",
            "   test accuracy: 0.905024\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181001165628/saved_model.pb\n",
            "Epoch 8 : Step 12496-14058 of 24992 took 1331.037133s\n",
            "   train loss: 0.363872\n",
            "   train accuracy: 0.926211\n",
            "   validation loss: 0.428061\n",
            "   validation accuracy: 0.906898\n",
            "   test loss: 0.429938\n",
            "   test accuracy: 0.906495\n",
            "Epoch 9 : Step 14058-15620 of 24992 took 1322.978894s\n",
            "   train loss: 0.332786\n",
            "   train accuracy: 0.929688\n",
            "   validation loss: 0.419902\n",
            "   validation accuracy: 0.905269\n",
            "   test loss: 0.421023\n",
            "   test accuracy: 0.904779\n",
            "Epoch 10 : Step 15620-17182 of 24992 took 1342.057805s\n",
            "   train loss: 0.308933\n",
            "   train accuracy: 0.933079\n",
            "   validation loss: 0.405887\n",
            "   validation accuracy: 0.906005\n",
            "   test loss: 0.409137\n",
            "   test accuracy: 0.906705\n",
            "Epoch 11 : Step 17182-18744 of 24992 took 1320.643944s\n",
            "   train loss: 0.290069\n",
            "   train accuracy: 0.936295\n",
            "   validation loss: 0.403193\n",
            "   validation accuracy: 0.906793\n",
            "   test loss: 0.404538\n",
            "   test accuracy: 0.906250\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181001183757/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-65e8a895123b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n\u001b[1;32m    119\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtrain_op_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         )\n\u001b[1;32m    122\u001b[0m         \u001b[0msum_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn_batch_a_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-65e8a895123b>\u001b[0m in \u001b[0;36mperform_minibatch\u001b[0;34m(run_list, X, y, batch_size, is_train)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 _, cost, accuracy = session.run(\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_batch_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch_a\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 )\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "UbFcV5edSMlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save the trained model"
      ]
    },
    {
      "metadata": {
        "id": "4wFMK2v6SMvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f5a97513-f377-4f12-f8a2-6d9bd0424264"
      },
      "cell_type": "code",
      "source": [
        "save_model()\n",
        "\n",
        "tfboard_file_writer.flush()\n",
        "tfboard_file_writer.close()\n",
        "\n",
        "session.close()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181001185330/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3e3lOIaaTd0s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert to tensorflow.js web model"
      ]
    },
    {
      "metadata": {
        "id": "CrdV5utoTeB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "4a531b09-1cd7-4167-d5d3-cde6a24a2aa5"
      },
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=tf_saved_model --output_node_names=\"model/y_output\" saved-model web-model\n",
        "!echo \"Current directory ->\"\n",
        "!ls -la\n",
        "!echo \"web-model directory ->\"\n",
        "!ls -la web-model\n",
        "!echo \"saved-model directory ->\"\n",
        "!ls -la saved-model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2018-10-01 18:53:43.874130: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2018-10-01 18:53:44.441452: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:402] Optimization results for grappler item: graph_to_optimize\n",
            "2018-10-01 18:53:44.441519: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   debug_stripper: Graph size after: 66 nodes (0), 67 edges (0), time = 0.059ms.\n",
            "2018-10-01 18:53:44.441571: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   model_pruner: Graph size after: 50 nodes (-16), 51 edges (-16), time = 0.309ms.\n",
            "2018-10-01 18:53:44.441597: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   constant folding: Graph size after: 30 nodes (-20), 29 edges (-22), time = 37.061ms.\n",
            "2018-10-01 18:53:44.441619: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   arithmetic_optimizer: Graph size after: 29 nodes (-1), 29 edges (0), time = 30.849ms.\n",
            "2018-10-01 18:53:44.441640: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   dependency_optimizer: Graph size after: 26 nodes (-3), 25 edges (-4), time = 0.545ms.\n",
            "2018-10-01 18:53:44.441661: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   model_pruner: Graph size after: 26 nodes (0), 25 edges (0), time = 0.336ms.\n",
            "2018-10-01 18:53:44.441684: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   constant folding: Graph size after: 26 nodes (0), 25 edges (0), time = 13.524ms.\n",
            "2018-10-01 18:53:44.441714: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   arithmetic_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 16ms.\n",
            "2018-10-01 18:53:44.441736: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   dependency_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.478ms.\n",
            "2018-10-01 18:53:44.441757: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   debug_stripper: Graph size after: 26 nodes (0), 25 edges (0), time = 0.12ms.\n",
            "2018-10-01 18:53:44.441778: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   model_pruner: Graph size after: 26 nodes (0), 25 edges (0), time = 0.202ms.\n",
            "2018-10-01 18:53:44.441799: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   constant folding: Graph size after: 26 nodes (0), 25 edges (0), time = 13.232ms.\n",
            "2018-10-01 18:53:44.441820: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   arithmetic_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 15.377ms.\n",
            "2018-10-01 18:53:44.441842: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   dependency_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.494ms.\n",
            "2018-10-01 18:53:44.441863: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   model_pruner: Graph size after: 26 nodes (0), 25 edges (0), time = 0.173ms.\n",
            "2018-10-01 18:53:44.441884: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   constant folding: Graph size after: 26 nodes (0), 25 edges (0), time = 13.281ms.\n",
            "2018-10-01 18:53:44.441905: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   arithmetic_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 15.636ms.\n",
            "2018-10-01 18:53:44.441926: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404]   dependency_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.443ms.\n",
            "Writing weight file web-model/tensorflowjs_model.pb...\n",
            "Current directory ->\n",
            "total 28\n",
            "drwxr-xr-x 1 root root 4096 Oct  1 18:53 .\n",
            "drwxr-xr-x 1 root root 4096 Oct  1 12:51 ..\n",
            "drwxr-xr-x 4 root root 4096 Sep 27 20:11 .config\n",
            "drwxr-xr-x 8 root root 4096 Oct  1 18:53 data\n",
            "drwxr-xr-x 2 root root 4096 Oct  1 13:34 logs\n",
            "drwxr-xr-x 2 root root 4096 Sep 27 20:32 sample_data\n",
            "lrwxrwxrwx 1 root root   31 Oct  1 18:53 saved-model -> data/saved-model-20181001185330\n",
            "drwxr-xr-x 2 root root 4096 Oct  1 18:53 web-model\n",
            "web-model directory ->\n",
            "total 5836\n",
            "drwxr-xr-x 2 root root    4096 Oct  1 18:53 .\n",
            "drwxr-xr-x 1 root root    4096 Oct  1 18:53 ..\n",
            "-rw-r--r-- 1 root root 4194304 Oct  1 18:53 group1-shard1of2\n",
            "-rw-r--r-- 1 root root 1343536 Oct  1 18:53 group1-shard2of2\n",
            "-rw-r--r-- 1 root root  418453 Oct  1 18:53 tensorflowjs_model.pb\n",
            "-rw-r--r-- 1 root root     706 Oct  1 18:53 weights_manifest.json\n",
            "saved-model directory ->\n",
            "lrwxrwxrwx 1 root root 31 Oct  1 18:53 saved-model -> data/saved-model-20181001185330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q8g244v4VZUj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Zip and download the models"
      ]
    },
    {
      "metadata": {
        "id": "0fzcYLP3VB97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ed890af9-09a6-452a-d9f6-6644ac5aa665"
      },
      "cell_type": "code",
      "source": [
        "!zip -r web-model.zip web-model\n",
        "!zip -r saved-model.zip saved-model\n",
        "!ls -la *.zip\n",
        "\n",
        "from google.colab import files\n",
        "files.download('web-model.zip')\n",
        "files.download('saved-model.zip')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: web-model/ (stored 0%)\n",
            "  adding: web-model/weights_manifest.json (deflated 71%)\n",
            "  adding: web-model/group1-shard1of2 (deflated 5%)\n",
            "  adding: web-model/group1-shard2of2 (deflated 4%)\n",
            "  adding: web-model/tensorflowjs_model.pb (deflated 8%)\n",
            "  adding: saved-model/ (stored 0%)\n",
            "  adding: saved-model/saved_model.pb (deflated 90%)\n",
            "  adding: saved-model/variables/ (stored 0%)\n",
            "  adding: saved-model/variables/variables.index (deflated 47%)\n",
            "  adding: saved-model/variables/variables.data-00000-of-00001 (deflated 5%)\n",
            "-rw-r--r-- 1 root root 15823619 Oct  1 18:53 saved-model.zip\n",
            "-rw-r--r-- 1 root root  5670175 Oct  1 18:53 web-model.zip\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}