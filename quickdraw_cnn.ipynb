{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickdraw-cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/quickdraw-cnn/blob/master/quickdraw_cnn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "qs0seV43MwNv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## \"You draw, I guess.\" is a POC project that uses CNN to recognize sketch drawings.\n",
        "### The CNN was trained to recognize 345 classes using <a href='https://github.com/googlecreativelab/quickdraw-dataset'>\"The Quick, Draw! Dataset\" </a> of Google awesome \"猜画小歌\" Wechat App."
      ]
    },
    {
      "metadata": {
        "id": "0_FT6bGsOY22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install dependent packages"
      ]
    },
    {
      "metadata": {
        "id": "EDJoFej9MZ83",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install \"tensorlayer>=1.10\"\n",
        "!pip install tensorflowjs\n",
        "!pip list|grep tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QV6AypOzOiws",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependences, and check if GPU is available"
      ]
    },
    {
      "metadata": {
        "id": "W4-1-Q-EOi8j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "from random import randint\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import *\n",
        "from tensorflow.python import debug as tfdebug\n",
        "\n",
        "\"\"\" Notice to put ```import matplotlib.pyplot``` after imports of tensorlayer, \n",
        "otherwise you will get below warning:\n",
        "\n",
        "This call to matplotlib.use() has no effect because the backend has already\n",
        "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
        "or matplotlib.backends is imported for the first time.\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('### device name: {} ###'.format(device_name))\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('*** GPU device not found ***')\n",
        "print('### Found GPU at: {} ###'.format(device_name))\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q27izcVnPcQu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the Quick, Draw dataset"
      ]
    },
    {
      "metadata": {
        "id": "QeDBh_7WPc0d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "working_directory = 'data'\n",
        "dataset_directory = 'data/quickdraw'\n",
        "# categories_filename = 'categories.txt'\n",
        "categories_file_url_source = 'https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/'\n",
        "# curl -v --socks5 127.0.0.1:1080 \"https://storage.cloud.google.com/quickdraw_dataset/full/numpy_bitmap/aircraft%20carrier.npy\"\n",
        "# npy_dataset_url_source = 'https://storage.cloud.google.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "npy_dataset_url_source = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "\n",
        "X = \"X\"\n",
        "y = \"y\"\n",
        "\n",
        "X_NUMPY_DTYPE = np.float32\n",
        "y_NUMPY_DTYPE = np.int64\n",
        "X_TF_DTYPE = tf.float32\n",
        "y_TF_DTYPE = tf.int32\n",
        "\n",
        "image_height = 28\n",
        "image_width = 28\n",
        "image_depth = 1\n",
        "image_size = image_height * image_width * image_depth\n",
        "input_layer_X_shape = [image_height, image_width, image_depth]\n",
        "input_layer_X_shape_batch = [-1, image_height, image_width, image_depth]\n",
        "\n",
        "mini_categories_filename = 'web/mini-categories.txt'\n",
        "n_category = 10  # The maximum of category number is up to 345\n",
        "n_train_example_per_category = 20000\n",
        "\n",
        "\n",
        "def print_dataset_shape(X_name, X, y_name, y):\n",
        "    print(X_name + '.shape ', X.shape, end='\\t,\\t')\n",
        "    print(y_name + '.shape ', y.shape)\n",
        "    print('%s.dtype %s\\t,\\t%s.dtype %s' % (X_name, X.dtype, y_name, y.dtype))\n",
        "\n",
        "\n",
        "def show_image(X, y, categories):\n",
        "    plt.imshow(X.reshape(image_height, image_width), cmap=\"gray\", interpolation='nearest')\n",
        "    plt.title(f\"{categories[y]}(label: {y})\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_quickdraw_dataset(\n",
        "    n_category=10, n_train_example_per_category=20000\n",
        "):\n",
        "    \"\"\" Download the quick draw data set. \"\"\"\n",
        "    n_validation_example_per_category = int(n_train_example_per_category / 0.7 * 0.2)\n",
        "    n_test_example_per_category = int(n_train_example_per_category / 0.7 * 0.1)\n",
        "\n",
        "    # Download the categories file\n",
        "    # tl.files.utils.maybe_download_and_extract(categories_filename, dataset_directory, categories_file_url_source)\n",
        "\n",
        "    tl.logging.info(\"Load or Download quick draw > {}\".format(dataset_directory))\n",
        "\n",
        "    train_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "    validation_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "    test_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "\n",
        "    # category_names = [line.rstrip('\\n') for line in open(f\"{dataset_directory}/{categories_filename}\")]\n",
        "    category_names = [line.rstrip('\\n') for line in open(mini_categories_filename)]\n",
        "    for category_index, category_name in enumerate(category_names):\n",
        "        if category_index == n_category:\n",
        "            break\n",
        "\n",
        "        category_names[category_index], _, _ = category_name.rpartition('=')\n",
        "        category_name = category_names[category_index]\n",
        "\n",
        "        filename = urllib.parse.quote(category_name) + '.npy'\n",
        "        tl.files.utils.maybe_download_and_extract(filename, dataset_directory, npy_dataset_url_source)\n",
        "\n",
        "        data = np.load(os.path.join(dataset_directory, filename))\n",
        "        size_per_category = data.shape[0]\n",
        "        labels = np.full(size_per_category, category_index)\n",
        "\n",
        "        print(f\"### Category '{category_name}' id:{category_index} dataset info ###\")\n",
        "        print_dataset_shape(\"data\", data, \"labels\", labels)\n",
        "\n",
        "        number_begin = 0\n",
        "        number_end = n_train_example_per_category\n",
        "        # train_set[X] = np.concatenate((train_set[X], data[number_begin: number_end, :]), axis=0)\n",
        "        train_set[X] = np.vstack((train_set[X], data[number_begin: number_end, :]))\n",
        "        train_set[y] = np.append(train_set[y], labels[number_begin: number_end])\n",
        "\n",
        "        number_begin += n_train_example_per_category\n",
        "        number_end += n_validation_example_per_category\n",
        "        # validation_set[X] = np.concatenate((validation_set[X], data[number_begin:number_end, :]), axis=0)\n",
        "        validation_set[X] = np.vstack((validation_set[X], data[number_begin:number_end, :]))\n",
        "        validation_set[y] = np.append(validation_set[y], labels[number_begin:number_end])\n",
        "\n",
        "        number_begin += n_validation_example_per_category\n",
        "        number_end += n_test_example_per_category\n",
        "        # test_set[X] = np.concatenate((test_set[X], data[number_begin:number_end, :]), axis=0)\n",
        "        test_set[X] = np.vstack((test_set[X], data[number_begin:number_end, :]))\n",
        "        test_set[y] = np.append(test_set[y], labels[number_begin:number_end])\n",
        "\n",
        "        print_dataset_shape(\"train_set[X]\", train_set[X], \"train_set[y]\", train_set[y])\n",
        "        print_dataset_shape(\"validation_set[X]\", validation_set[X], \"validation_set[y]\", validation_set[y])\n",
        "        print_dataset_shape(\"test_set[X]\", test_set[X], \"test_set[y]\", test_set[y])\n",
        "\n",
        "    # Randomize the dataset\n",
        "    size_per_set = train_set[X].shape[0]\n",
        "    permutation = np.random.permutation(size_per_set)\n",
        "    train_set[X] = train_set[X][permutation, :]\n",
        "    train_set[y] = train_set[y][permutation]\n",
        "\n",
        "    size_per_set = validation_set[X].shape[0]\n",
        "    permutation = np.random.permutation(size_per_set)\n",
        "    validation_set[X] = validation_set[X][permutation, :]\n",
        "    validation_set[y] = validation_set[y][permutation]\n",
        "\n",
        "    size_per_set = test_set[X].shape[0]\n",
        "    permutation = np.random.permutation(size_per_set)\n",
        "    test_set[X] = test_set[X][permutation, :]\n",
        "    test_set[y] = test_set[y][permutation]\n",
        "\n",
        "    # Reshape for CNN input\n",
        "    train_set[X] = train_set[X].reshape(input_layer_X_shape_batch)\n",
        "    validation_set[X] = validation_set[X].reshape(input_layer_X_shape_batch)\n",
        "    test_set[X] = test_set[X].reshape(input_layer_X_shape_batch)\n",
        "\n",
        "    # The original grayscale image is 'black background (x==0) and gray~white (0< x <=255) brush'\n",
        "    # Because the CNN model doesn't need to learn the grayscale values and it only needs to\n",
        "    # learn the strokes, we normalize it to 'white background (x==1) and block (x==0) brush'.\n",
        "    train_set[X] = 1.0 - np.ceil(train_set[X] / 255.0)\n",
        "    validation_set[X] = 1.0 - np.ceil(validation_set[X] / 255.0)\n",
        "    test_set[X] = 1.0 - np.ceil(test_set[X] / 255.0)\n",
        "\n",
        "    return category_names, train_set, validation_set, test_set\n",
        "\n",
        "\n",
        "# Open TensorBoard logs writer\n",
        "tfboard_file_writer = tf.summary.FileWriter('logs')\n",
        "\n",
        "# Download data\n",
        "category_names, train_set, validation_set, test_set = load_quickdraw_dataset(n_category, n_train_example_per_category)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LejFWzHKP2Dg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Function: Network model definition"
      ]
    },
    {
      "metadata": {
        "id": "d5TwN6-sP1eQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_batch_normalization(X_batch, y_batch, output_units, reuse, is_train):\n",
        "    \"\"\" Define the network model \"\"\"\n",
        "    W_init1 = tf.truncated_normal_initializer(stddev=5e-2)\n",
        "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
        "    bias_init = tf.constant_initializer(value=0.1)\n",
        "\n",
        "    with tf.variable_scope(\"model\", reuse=reuse):\n",
        "        net = InputLayer(X_batch, name='input')\n",
        "        net = Conv2d(net, 64, (3, 3), (1, 1), padding='SAME',\n",
        "                     W_init=W_init1, b_init=None, name='cnn1')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch1')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
        "\n",
        "        net = Conv2d(net, 64, (3, 3), (1, 1), padding='SAME',\n",
        "                     W_init=W_init1, b_init=None, name='cnn2')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch2')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
        "\n",
        "        net = FlattenLayer(net, name='flatten')\n",
        "        net = DenseLayer(net, 384, act=tf.nn.relu,\n",
        "                         W_init=W_init2, b_init=bias_init, name='d1relu')\n",
        "        net = DenseLayer(net, 192, act=tf.nn.relu,\n",
        "                         W_init=W_init2, b_init=bias_init, name='d2relu')\n",
        "        # The softmax() is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
        "        # speed up computation, so we use identity here.\n",
        "        # see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
        "        net = DenseLayer(net, n_units=output_units, act=None,\n",
        "                         W_init=W_init2, name='output')\n",
        "\n",
        "        y_prediction_batch_without_softmax = net.outputs\n",
        "\n",
        "        # For inference by using this model\n",
        "        # y_output = tf.argmax(tf.nn.softmax(y_prediction_batch_without_softmax), 1)\n",
        "        y_output = tf.nn.softmax(y_prediction_batch_without_softmax, name=\"y_output\")\n",
        "\n",
        "        ce = tl.cost.cross_entropy(y_prediction_batch_without_softmax, y_batch, name='cost')\n",
        "\n",
        "        \"\"\" 需给后面的全连接层引入L2 normalization，惩罚模型的复杂度，避免overfitting \"\"\"\n",
        "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
        "        L2 = 0\n",
        "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
        "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
        "        # 加上L2模型复杂度惩罚项后，得到最终真正的cost\n",
        "        cost = ce + L2\n",
        "\n",
        "        correct_prediction = tf.equal(tf.cast(tf.argmax(y_prediction_batch_without_softmax, 1), y_TF_DTYPE), y_batch)\n",
        "        # correct_prediction = tf.Print(correct_prediction, [correct_prediction], \"correct_prediction: \")\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        return net, cost, accuracy, y_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jpISk8SQqCO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Train, validate and test"
      ]
    },
    {
      "metadata": {
        "id": "3iOAaIdbQoQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train, validate and test\n",
        "batch_size = 128\n",
        "n_epoch = 16\n",
        "n_step_per_epoch = int(len(train_set[y]) / batch_size)\n",
        "n_step = n_epoch * n_step_per_epoch\n",
        "print_freq = 1\n",
        "checkpoint_freq = 4\n",
        "learning_rate = 0.0001\n",
        "\n",
        "model_ckpt_file_name = os.path.join(working_directory, \"checkpoint\", \"model-quickdraw-cnn.ckpt\")\n",
        "resume = True  # load model, resume from previous checkpoint?\n",
        "\n",
        "\n",
        "def distort_fn(X, is_train=False):\n",
        "    # print('begin', X.shape, np.min(X), np.max(X))\n",
        "\n",
        "    if is_train == True:\n",
        "        # 1. Randomly flip the image horizontally.\n",
        "        X = tf.image.random_flip_left_right(X)\n",
        "\n",
        "    # X = tf.image.per_image_standardization(X)\n",
        "\n",
        "    # print('after norm', X.shape, np.min(X), np.max(X), np.mean(X))\n",
        "    return X\n",
        "\n",
        "\n",
        "def save_model():\n",
        "    model_type = \"saved-model\"\n",
        "    latest_model_directory = f'{model_type}-{time.strftime(\"%Y%m%d%H%M%S\", time.localtime())}'\n",
        "    saved_model_directory = os.path.join(working_directory, latest_model_directory)\n",
        "    if not os.path.exists(saved_model_directory):\n",
        "        tf.saved_model.simple_save(session, saved_model_directory,\n",
        "                                   inputs={\"X\": X_batch_ph},\n",
        "                                   outputs={\"y_output\": y_prediction_})\n",
        "        dist_directory = os.path.join(\".\", model_type)\n",
        "        if os.path.exists(dist_directory):\n",
        "            os.remove(dist_directory)\n",
        "        os.symlink(saved_model_directory, dist_directory, target_is_directory=True)\n",
        "\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "    session = tf.Session(config=config)\n",
        "\n",
        "    #\n",
        "    # Connect to tfdbg dashboard by ```http://localhost:6006#debugger```\n",
        "    # when the following command is issued.\n",
        "    #\n",
        "    # ```bash\n",
        "    # $ tensorboard --logdir logs --port 6006 --debugger_port 6064\n",
        "    # ```\n",
        "    #\n",
        "    # session = tfdebug.TensorBoardDebugWrapperSession(session, \"albert-mbp.local:6064\")\n",
        "\n",
        "    X_batch_ph = tf.placeholder(dtype=X_TF_DTYPE, shape=[None, image_height, image_width, image_depth], name='X_batch')\n",
        "    y_batch_ph = tf.placeholder(dtype=y_TF_DTYPE, shape=[None], name='y_batch')\n",
        "    # X_batch_ph = tf.placeholder(dtype=X_TF_DTYPE, shape=[batch_size, image_height, image_width, image_depth], name='X')\n",
        "    # y_batch_ph = tf.placeholder(dtype=y_TF_DTYPE, shape=[batch_size], name='y')\n",
        "\n",
        "    def perform_minibatch(run_list, X, y, batch_size, is_train=False):\n",
        "        n_batch, sum_loss, sum_accuracy = 0, 0, 0\n",
        "        for X_batch_a, y_batch_a in tl.iterate.minibatches(X, y, batch_size, shuffle=is_train):\n",
        "            # data augmentation for training\n",
        "            # X_batch_a = tl.prepro.threading_data(X_batch_a, fn=distort_fn, is_train=is_train)\n",
        "\n",
        "            cost, accuracy = 0, 0\n",
        "            if is_train:\n",
        "                _, cost, accuracy = session.run(\n",
        "                    run_list, feed_dict={X_batch_ph: X_batch_a, y_batch_ph: y_batch_a}\n",
        "                )\n",
        "            else:\n",
        "                cost, accuracy = session.run(\n",
        "                    run_list, feed_dict={X_batch_ph: X_batch_a, y_batch_ph: y_batch_a}\n",
        "                )\n",
        "\n",
        "            sum_loss += cost\n",
        "            sum_accuracy += accuracy\n",
        "            n_batch += 1\n",
        "        return n_batch, sum_loss, sum_accuracy\n",
        "\n",
        "\n",
        "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
        "        # Build the model\n",
        "        print(\"### Train Network model ###\")\n",
        "        network_, cost_, accuracy_, y_prediction_ = model_batch_normalization(\n",
        "            X_batch_ph, y_batch_ph, n_category, reuse=None, is_train=True\n",
        "        )\n",
        "        print(\"### Reuse this Train Network model for validation and test ###\")\n",
        "        _, cost_test_, accuracy_test_, y_prediction_test_ = model_batch_normalization(\n",
        "            X_batch_ph, y_batch_ph, n_category, reuse=True, is_train=False\n",
        "        )\n",
        "\n",
        "    # Define the training optimizer\n",
        "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
        "        train_op_ = tf.train.AdamOptimizer(learning_rate).minimize(cost_)\n",
        "\n",
        "    tl.layers.initialize_global_variables(session)\n",
        "\n",
        "    # Attach the graph for TensorBoard writer\n",
        "    # tfboard_file_writer.add_graph(tf.get_default_graph())\n",
        "    tfboard_file_writer.add_graph(session.graph)\n",
        "\n",
        "    if resume and os.path.isfile(model_ckpt_file_name):\n",
        "        print(\"Load existing model \" + \"!\" * 10)\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(session, model_ckpt_file_name)\n",
        "\n",
        "    print(\"### Network parameters ###\")\n",
        "    network_.print_params(False)\n",
        "    print(\"### Network layers ###\")\n",
        "    network_.print_layers()\n",
        "\n",
        "    print('   learning_rate: %f' % learning_rate)\n",
        "    print('   batch_size: %d' % batch_size)\n",
        "    print('   n_epoch: %d, step in an epoch: %d, total n_step: %d' % (n_epoch, n_step_per_epoch, n_step))\n",
        "\n",
        "    step, sum_batch, sum_loss, sum_accuracy = 0, 0, 0, 0\n",
        "    for epoch in range(n_epoch):\n",
        "        start_time = time.time()\n",
        "\n",
        "        n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "            [train_op_, cost_, accuracy_],\n",
        "            train_set[X], train_set[y], batch_size, is_train=True\n",
        "        )\n",
        "        sum_batch += n_batch_a_epoch\n",
        "        sum_loss += cost_a_epoch\n",
        "        sum_accuracy += accuracy_a_epoch\n",
        "        step += n_batch_a_epoch\n",
        "\n",
        "        assert n_batch_a_epoch == n_step_per_epoch\n",
        "\n",
        "        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
        "            print(\"Epoch %d : Step %d-%d of %d took %fs\" %\n",
        "                  (epoch + 1, step - n_step_per_epoch, step, n_step, time.time() - start_time))\n",
        "            print(\"   train loss: %f\" % (sum_loss / sum_batch))\n",
        "            print(\"   train accuracy: %f\" % (sum_accuracy / sum_batch))\n",
        "            sum_batch, sum_loss, sum_accuracy = 0, 0, 0\n",
        "\n",
        "            n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "                [cost_test_, accuracy_test_],\n",
        "                validation_set[X], validation_set[y], batch_size\n",
        "            )\n",
        "            print(\"   validation loss: %f\" % (cost_a_epoch / n_batch_a_epoch))\n",
        "            print(\"   validation accuracy: %f\" % (accuracy_a_epoch / n_batch_a_epoch))\n",
        "\n",
        "            n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "                [cost_test_, accuracy_test_],\n",
        "                test_set[X], test_set[y], batch_size\n",
        "            )\n",
        "            print(\"   test loss: %f\" % (cost_a_epoch / n_batch_a_epoch))\n",
        "            print(\"   test accuracy: %f\" % (accuracy_a_epoch / n_batch_a_epoch))\n",
        "\n",
        "        # Save model when checkpoint\n",
        "        if (epoch + 1) % checkpoint_freq == 0:\n",
        "            print(\"Saving checkpoint... \" + \"!\" * 10)\n",
        "            saver = tf.train.Saver()\n",
        "            save_path = saver.save(session, model_ckpt_file_name)\n",
        "            print(\"Saving model... \" + \"!\" * 10)\n",
        "            save_model()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UbFcV5edSMlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save the trained model"
      ]
    },
    {
      "metadata": {
        "id": "4wFMK2v6SMvD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_model()\n",
        "\n",
        "tfboard_file_writer.flush()\n",
        "tfboard_file_writer.close()\n",
        "\n",
        "session.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3e3lOIaaTd0s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert to TensorFlow.js web model"
      ]
    },
    {
      "metadata": {
        "id": "CrdV5utoTeB9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=tf_saved_model --output_node_names=\"model/y_output\" saved-model web-model\n",
        "!echo \"Current directory ->\"\n",
        "!ls -la\n",
        "!echo \"web-model directory ->\"\n",
        "!ls -la web-model\n",
        "!echo \"saved-model directory ->\"\n",
        "!ls -la saved-model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8g244v4VZUj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Zip and download the models"
      ]
    },
    {
      "metadata": {
        "id": "0fzcYLP3VB97",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!zip -r web-model.zip web-model\n",
        "!zip -r saved-model.zip saved-model\n",
        "!ls -la *.zip\n",
        "\n",
        "from google.colab import files\n",
        "files.download('web-model.zip')\n",
        "files.download('saved-model.zip')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}