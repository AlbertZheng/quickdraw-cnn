{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickdraw-cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/quickdraw-cnn/blob/master/quickdraw_cnn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "qs0seV43MwNv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## \"You draw, I guess.\" is a MVP (Minimium Viable Product) that uses CNN to recognize the sketch drawings on web canvas.\n",
        "### The CNN was trained to recognize 10 classes using <a href='https://github.com/googlecreativelab/quickdraw-dataset'>\"The Quick, Draw! Dataset\" </a> of Google awesome \"猜画小歌\" Wechat App."
      ]
    },
    {
      "metadata": {
        "id": "0_FT6bGsOY22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install dependent packages"
      ]
    },
    {
      "metadata": {
        "id": "EDJoFej9MZ83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2111
        },
        "outputId": "bd1e23c0-6b2e-4c3f-a3a5-cbb4c1e5ff89"
      },
      "cell_type": "code",
      "source": [
        "!pip install \"tensorlayer>=1.10\"\n",
        "!pip install tensorflowjs\n",
        "!pip list|grep tensor"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorlayer>=1.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/e2/736458723564163cfd87e3a9719a9fdece9011429bf556fb910d3691352e/tensorlayer-1.10.1-py2.py3-none-any.whl (313kB)\n",
            "\u001b[K    100% |████████████████████████████████| 317kB 6.1MB/s \n",
            "\u001b[?25hCollecting scipy<1.2,>=1.1 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 31.2MB 1.2MB/s \n",
            "\u001b[?25hCollecting scikit-image<0.15,>=0.14 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/90/553120309c53bdfca25c9c50769ae40a538a90c24db8c082468aec898d00/scikit_image-0.14.1-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 25.3MB 1.7MB/s \n",
            "\u001b[?25hCollecting tqdm<4.26,>=4.23 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/e0/52b2faaef4fd87f86eb8a8f1afa2cd6eb11146822033e29c04ac48ada32c/tqdm-4.25.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (0.19.2)\n",
            "Requirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (1.10.11)\n",
            "Collecting matplotlib<2.3,>=2.2 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.6MB 4.4MB/s \n",
            "\u001b[?25hCollecting lxml<4.3,>=4.2 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 4.2MB/s \n",
            "\u001b[?25hCollecting imageio<2.5,>=2.3 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b4/cbb592964dfd71a9de6a5b08f882fd334fb99ae09ddc82081dbb2f718c81/imageio-2.4.1.tar.gz (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 6.1MB/s \n",
            "\u001b[?25hCollecting progressbar2<3.39,>=3.38 (from tensorlayer>=1.10)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
            "Collecting requests<2.20,>=2.19 (from tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 25.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (1.14.6)\n",
            "Collecting pillow>=4.3.0 (from scikit-image<0.15,>=0.14->tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (1.0.1)\n",
            "Collecting dask[array]>=0.9.0 (from scikit-image<0.15,>=0.14->tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/73/8ffed9140e343455427e92e6a32c354e9acdbef0a23d0e8d6c336d4947e5/dask-0.19.4-py2.py3-none-any.whl (674kB)\n",
            "\u001b[K    100% |████████████████████████████████| 675kB 14.3MB/s \n",
            "\u001b[?25hCollecting cloudpickle>=0.2.1 (from scikit-image<0.15,>=0.14->tensorlayer>=1.10)\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/87/7b7ef3038b4783911e3fdecb5c566e3a817ce3e890e164fc174c088edb1e/cloudpickle-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (1.11.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (0.10.0)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib<2.3,>=2.2->tensorlayer>=1.10)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
            "\u001b[K    100% |████████████████████████████████| 952kB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2.5.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2018.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2.2.2)\n",
            "Collecting python-utils>=2.3.0 (from progressbar2<3.39,>=3.38->tensorlayer>=1.10)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (2018.8.24)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (2.6)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer>=1.10) (0.9.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer>=1.10) (4.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer>=1.10) (39.1.0)\n",
            "Building wheels for collected packages: imageio\n",
            "  Running setup.py bdist_wheel for imageio ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e0/43/31/605de9372ceaf657f152d3d5e82f42cf265d81db8bbe63cde1\n",
            "Successfully built imageio\n",
            "Installing collected packages: scipy, pillow, dask, cloudpickle, kiwisolver, matplotlib, scikit-image, tqdm, lxml, imageio, python-utils, progressbar2, requests, tensorlayer\n",
            "  Found existing installation: scipy 0.19.1\n",
            "    Uninstalling scipy-0.19.1:\n",
            "      Successfully uninstalled scipy-0.19.1\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "  Found existing installation: matplotlib 2.1.2\n",
            "    Uninstalling matplotlib-2.1.2:\n",
            "      Successfully uninstalled matplotlib-2.1.2\n",
            "  Found existing installation: scikit-image 0.13.1\n",
            "    Uninstalling scikit-image-0.13.1:\n",
            "      Successfully uninstalled scikit-image-0.13.1\n",
            "  Found existing installation: tqdm 4.26.0\n",
            "    Uninstalling tqdm-4.26.0:\n",
            "      Successfully uninstalled tqdm-4.26.0\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "Successfully installed cloudpickle-0.6.1 dask-0.19.4 imageio-2.4.1 kiwisolver-1.0.1 lxml-4.2.5 matplotlib-2.2.3 pillow-5.3.0 progressbar2-3.38.0 python-utils-2.3.0 requests-2.19.1 scikit-image-0.14.1 scipy-1.1.0 tensorlayer-1.10.1 tqdm-4.25.0\n",
            "Collecting tensorflowjs\n",
            "  Downloading https://files.pythonhosted.org/packages/43/b2/d09672c18d6bbaa7105f74f11f6f88f0dadbce6ea0691a4509b234c16a2c/tensorflowjs-0.6.4-py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow==1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.11.0)\n",
            "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.8.0)\n",
            "Collecting keras==2.2.2 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/7d/b1dedde8af99bd82f20ed7e9697aac0597de3049b1f786aa2aac3b9bd4da/Keras-2.2.2-py2.py3-none-any.whl (299kB)\n",
            "\u001b[K    100% |████████████████████████████████| 307kB 5.5MB/s \n",
            "\u001b[?25hCollecting numpy==1.15.1 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/94/7049fed8373c52839c8cde619acaf2c9b83082b935e5aa8c0fa27a4a8bcc/numpy-1.15.1-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.9MB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.11.0)\n",
            "Requirement already satisfied: tensorflow-hub==0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.1.1)\n",
            "Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (1.11.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (0.32.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (0.5.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (1.0.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (1.0.5)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (3.6.1)\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (39.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2->tensorflowjs) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0->tensorflowjs) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0->tensorflowjs) (3.0.1)\n",
            "\u001b[31mkeras 2.2.2 has requirement keras-applications==1.0.4, but you'll have keras-applications 1.0.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mkeras 2.2.2 has requirement keras-preprocessing==1.0.2, but you'll have keras-preprocessing 1.0.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, keras, tensorflowjs\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "  Found existing installation: Keras 2.1.6\n",
            "    Uninstalling Keras-2.1.6:\n",
            "      Successfully uninstalled Keras-2.1.6\n",
            "Successfully installed keras-2.2.2 numpy-1.15.1 tensorflowjs-0.6.4\n",
            "tensorboard              1.11.0   \n",
            "tensorflow               1.11.0   \n",
            "tensorflow-hub           0.1.1    \n",
            "tensorflowjs             0.6.4    \n",
            "tensorlayer              1.10.1   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QV6AypOzOiws",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependences, and check if GPU is available"
      ]
    },
    {
      "metadata": {
        "id": "W4-1-Q-EOi8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2f513fb2-8ce3-45ad-bd90-604dfe27239b"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "from random import randint\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import *\n",
        "from tensorflow.python import debug as tfdebug\n",
        "\n",
        "\"\"\" Notice to put ```import matplotlib.pyplot``` after imports of tensorlayer, \n",
        "otherwise you will get below warning:\n",
        "\n",
        "This call to matplotlib.use() has no effect because the backend has already\n",
        "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
        "or matplotlib.backends is imported for the first time.\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('### device name: {} ###'.format(device_name))\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('*** GPU device not found ***')\n",
        "print('### Found GPU at: {} ###'.format(device_name))\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### device name: /device:GPU:0 ###\n",
            "### Found GPU at: /device:GPU:0 ###\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q27izcVnPcQu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the Quick, Draw dataset"
      ]
    },
    {
      "metadata": {
        "id": "QeDBh_7WPc0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1564
        },
        "outputId": "15c4081b-e395-4c5d-852a-3f0fb31a11d3"
      },
      "cell_type": "code",
      "source": [
        "working_directory = 'data'\n",
        "dataset_directory = 'data/quickdraw'\n",
        "# categories_filename = 'categories.txt'\n",
        "# categories_file_url_source = 'https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/'\n",
        "\n",
        "# npy_dataset_url_source = 'https://storage.cloud.google.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "npy_dataset_url_source = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "\n",
        "X = \"X\"\n",
        "y = \"y\"\n",
        "\n",
        "X_NUMPY_DTYPE = np.float32\n",
        "y_NUMPY_DTYPE = np.int64\n",
        "X_TF_DTYPE = tf.float32\n",
        "y_TF_DTYPE = tf.int32\n",
        "\n",
        "image_height = 28\n",
        "image_width = 28\n",
        "image_depth = 1\n",
        "image_size = image_height * image_width * image_depth\n",
        "input_layer_X_shape = [image_height, image_width, image_depth]\n",
        "input_layer_X_shape_batch = [-1, image_height, image_width, image_depth]\n",
        "\n",
        "mini_categories_file_url_source = 'https://raw.githubusercontent.com/AlbertZheng/quickdraw-cnn/master/web/'\n",
        "mini_categories_filename = 'mini-categories.txt'\n",
        "n_category = 10  # The maximum of category number is up to 345\n",
        "n_train_example_per_category = 100000\n",
        "\n",
        "\n",
        "def print_dataset_shape(X_name, X, y_name, y):\n",
        "    print(X_name + '.shape ', X.shape, end='\\t,\\t')\n",
        "    print(y_name + '.shape ', y.shape)\n",
        "    print('%s.dtype %s\\t,\\t%s.dtype %s' % (X_name, X.dtype, y_name, y.dtype))\n",
        "\n",
        "\n",
        "def show_image(X, y, categories):\n",
        "    plt.imshow(X.reshape(image_height, image_width), cmap=\"gray\", interpolation='nearest')\n",
        "    plt.title(f\"{categories[y]}(label: {y})\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_quickdraw_dataset(\n",
        "    n_category=10, n_train_example_per_category=20000\n",
        "):\n",
        "    \"\"\" Download the quick draw data set. \"\"\"\n",
        "    n_validation_example_per_category = int(n_train_example_per_category / 0.7 * 0.2)\n",
        "    n_test_example_per_category = int(n_train_example_per_category / 0.7 * 0.1)\n",
        "\n",
        "    # Download the categories file\n",
        "    tl.files.utils.maybe_download_and_extract(mini_categories_filename, dataset_directory, mini_categories_file_url_source)\n",
        "\n",
        "    tl.logging.info(\"Load or Download quick draw > {}\".format(dataset_directory))\n",
        "\n",
        "    train_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "    validation_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "    test_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "\n",
        "    category_names = [line.rstrip('\\n') for line in open(f\"{dataset_directory}/{mini_categories_filename}\")]\n",
        "    for category_index, category_name in enumerate(category_names):\n",
        "        if category_index == n_category:\n",
        "            break\n",
        "\n",
        "        category_names[category_index], _, _ = category_name.rpartition('=')\n",
        "        category_name = category_names[category_index]\n",
        "\n",
        "        filename = urllib.parse.quote(category_name) + '.npy'\n",
        "        tl.files.utils.maybe_download_and_extract(filename, dataset_directory, npy_dataset_url_source)\n",
        "\n",
        "        data = np.load(os.path.join(dataset_directory, filename))\n",
        "        size_per_category = data.shape[0]\n",
        "        labels = np.full(size_per_category, category_index)\n",
        "\n",
        "        print(f\"### Category '{category_name}' id:{category_index} dataset info ###\")\n",
        "        print_dataset_shape(\"data\", data, \"labels\", labels)\n",
        "\n",
        "        number_begin = 0\n",
        "        number_end = n_train_example_per_category\n",
        "        # train_set[X] = np.concatenate((train_set[X], data[number_begin: number_end, :]), axis=0)\n",
        "        train_set[X] = np.vstack((train_set[X], data[number_begin: number_end, :]))\n",
        "        train_set[y] = np.append(train_set[y], labels[number_begin: number_end])\n",
        "\n",
        "        number_begin += n_train_example_per_category\n",
        "        number_end += n_validation_example_per_category\n",
        "        # validation_set[X] = np.concatenate((validation_set[X], data[number_begin:number_end, :]), axis=0)\n",
        "        validation_set[X] = np.vstack((validation_set[X], data[number_begin:number_end, :]))\n",
        "        validation_set[y] = np.append(validation_set[y], labels[number_begin:number_end])\n",
        "\n",
        "        number_begin += n_validation_example_per_category\n",
        "        number_end += n_test_example_per_category\n",
        "        # test_set[X] = np.concatenate((test_set[X], data[number_begin:number_end, :]), axis=0)\n",
        "        test_set[X] = np.vstack((test_set[X], data[number_begin:number_end, :]))\n",
        "        test_set[y] = np.append(test_set[y], labels[number_begin:number_end])\n",
        "\n",
        "        print_dataset_shape(\"train_set[X]\", train_set[X], \"train_set[y]\", train_set[y])\n",
        "        print_dataset_shape(\"validation_set[X]\", validation_set[X], \"validation_set[y]\", validation_set[y])\n",
        "        print_dataset_shape(\"test_set[X]\", test_set[X], \"test_set[y]\", test_set[y])\n",
        "\n",
        "    # Randomize the dataset\n",
        "    size_per_set = train_set[X].shape[0]\n",
        "    permutation = np.random.permutation(size_per_set)\n",
        "    train_set[X] = train_set[X][permutation, :]\n",
        "    train_set[y] = train_set[y][permutation]\n",
        "\n",
        "    size_per_set = validation_set[X].shape[0]\n",
        "    permutation = np.random.permutation(size_per_set)\n",
        "    validation_set[X] = validation_set[X][permutation, :]\n",
        "    validation_set[y] = validation_set[y][permutation]\n",
        "\n",
        "    size_per_set = test_set[X].shape[0]\n",
        "    permutation = np.random.permutation(size_per_set)\n",
        "    test_set[X] = test_set[X][permutation, :]\n",
        "    test_set[y] = test_set[y][permutation]\n",
        "\n",
        "    # Reshape for CNN input\n",
        "    train_set[X] = train_set[X].reshape(input_layer_X_shape_batch)\n",
        "    validation_set[X] = validation_set[X].reshape(input_layer_X_shape_batch)\n",
        "    test_set[X] = test_set[X].reshape(input_layer_X_shape_batch)\n",
        "\n",
        "    # The original grayscale image is 'black background (x==0) and gray~white (0< x <=255) brush'\n",
        "    # Because the CNN model doesn't need to learn the grayscale values and it only needs to\n",
        "    # learn the strokes, we normalize it to 'white background (x==1) and block (x==0) brush'.\n",
        "    train_set[X] = 1.0 - np.ceil(train_set[X] / 255.0)\n",
        "    validation_set[X] = 1.0 - np.ceil(validation_set[X] / 255.0)\n",
        "    test_set[X] = 1.0 - np.ceil(test_set[X] / 255.0)\n",
        "\n",
        "    return category_names, train_set, validation_set, test_set\n",
        "\n",
        "\n",
        "# Open TensorBoard logs writer\n",
        "tfboard_file_writer = tf.summary.FileWriter('logs')\n",
        "\n",
        "# Download data\n",
        "category_names, train_set, validation_set, test_set = load_quickdraw_dataset(n_category, n_train_example_per_category)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TL] Load or Download quick draw > data/quickdraw\n",
            "### Category 'airplane' id:0 dataset info ###\n",
            "data.shape  (151623, 784)\t,\tlabels.shape  (151623,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (100000, 784)\t,\ttrain_set[y].shape  (100000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (28571, 784)\t,\tvalidation_set[y].shape  (28571,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (14285, 784)\t,\ttest_set[y].shape  (14285,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'alarm clock' id:1 dataset info ###\n",
            "data.shape  (123399, 784)\t,\tlabels.shape  (123399,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (200000, 784)\t,\ttrain_set[y].shape  (200000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (51970, 784)\t,\tvalidation_set[y].shape  (51970,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (14285, 784)\t,\ttest_set[y].shape  (14285,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'apple' id:2 dataset info ###\n",
            "data.shape  (144722, 784)\t,\tlabels.shape  (144722,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (300000, 784)\t,\ttrain_set[y].shape  (300000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (80541, 784)\t,\tvalidation_set[y].shape  (80541,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (28570, 784)\t,\ttest_set[y].shape  (28570,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'bicycle' id:3 dataset info ###\n",
            "data.shape  (126527, 784)\t,\tlabels.shape  (126527,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (400000, 784)\t,\ttrain_set[y].shape  (400000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (107068, 784)\t,\tvalidation_set[y].shape  (107068,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (28570, 784)\t,\ttest_set[y].shape  (28570,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'car' id:4 dataset info ###\n",
            "data.shape  (182764, 784)\t,\tlabels.shape  (182764,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (500000, 784)\t,\ttrain_set[y].shape  (500000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (135639, 784)\t,\tvalidation_set[y].shape  (135639,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (42855, 784)\t,\ttest_set[y].shape  (42855,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'cloud' id:5 dataset info ###\n",
            "data.shape  (120265, 784)\t,\tlabels.shape  (120265,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (600000, 784)\t,\ttrain_set[y].shape  (600000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (155904, 784)\t,\tvalidation_set[y].shape  (155904,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (42855, 784)\t,\ttest_set[y].shape  (42855,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'cup' id:6 dataset info ###\n",
            "data.shape  (130721, 784)\t,\tlabels.shape  (130721,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (700000, 784)\t,\ttrain_set[y].shape  (700000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (184475, 784)\t,\tvalidation_set[y].shape  (184475,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (45005, 784)\t,\ttest_set[y].shape  (45005,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'dog' id:7 dataset info ###\n",
            "data.shape  (152159, 784)\t,\tlabels.shape  (152159,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (800000, 784)\t,\ttrain_set[y].shape  (800000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (213046, 784)\t,\tvalidation_set[y].shape  (213046,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (59290, 784)\t,\ttest_set[y].shape  (59290,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'eyeglasses' id:8 dataset info ###\n",
            "data.shape  (225762, 784)\t,\tlabels.shape  (225762,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (900000, 784)\t,\ttrain_set[y].shape  (900000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (241617, 784)\t,\tvalidation_set[y].shape  (241617,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (73575, 784)\t,\ttest_set[y].shape  (73575,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 't-shirt' id:9 dataset info ###\n",
            "data.shape  (125233, 784)\t,\tlabels.shape  (125233,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (1000000, 784)\t,\ttrain_set[y].shape  (1000000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (266850, 784)\t,\tvalidation_set[y].shape  (266850,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (73575, 784)\t,\ttest_set[y].shape  (73575,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LejFWzHKP2Dg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Function: Network model definition"
      ]
    },
    {
      "metadata": {
        "id": "d5TwN6-sP1eQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_batch_normalization(X_batch, y_batch, output_units, reuse, is_train):\n",
        "    \"\"\" Define the network model \"\"\"\n",
        "    W_init1 = tf.truncated_normal_initializer(stddev=5e-2)\n",
        "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
        "    bias_init = tf.constant_initializer(value=0.1)\n",
        "\n",
        "    with tf.variable_scope(\"model\", reuse=reuse):\n",
        "        net = InputLayer(X_batch, name='input')\n",
        "        net = Conv2d(net, 128, (5, 5), (1, 1), padding='SAME',\n",
        "                     W_init=W_init1, b_init=None, name='cnn1')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch1')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
        "\n",
        "        net = Conv2d(net, 256, (5, 5), (1, 1), padding='SAME',\n",
        "                     W_init=W_init1, b_init=None, name='cnn2')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch2')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
        "\n",
        "        net = FlattenLayer(net, name='flatten')\n",
        "        net = DenseLayer(net, 384, act=tf.nn.relu,\n",
        "                         W_init=W_init2, b_init=bias_init, name='d1relu')\n",
        "        net = DenseLayer(net, 192, act=tf.nn.relu,\n",
        "                         W_init=W_init2, b_init=bias_init, name='d2relu')\n",
        "        # The softmax() is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
        "        # speed up computation, so we use identity here.\n",
        "        # see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
        "        net = DenseLayer(net, n_units=output_units, act=None,\n",
        "                         W_init=W_init2, name='output')\n",
        "\n",
        "        y_prediction_batch_without_softmax = net.outputs\n",
        "\n",
        "        # For inference by using this model\n",
        "        # y_output = tf.argmax(tf.nn.softmax(y_prediction_batch_without_softmax), 1)\n",
        "        y_output = tf.nn.softmax(y_prediction_batch_without_softmax, name=\"y_output\")\n",
        "\n",
        "        ce = tl.cost.cross_entropy(y_prediction_batch_without_softmax, y_batch, name='cost')\n",
        "\n",
        "        \"\"\" 需给后面的全连接层引入L2 normalization，惩罚模型的复杂度，避免overfitting \"\"\"\n",
        "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
        "        L2 = 0\n",
        "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
        "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
        "        # 加上L2模型复杂度惩罚项后，得到最终真正的cost\n",
        "        cost = ce + L2\n",
        "\n",
        "        correct_prediction = tf.equal(tf.cast(tf.argmax(y_prediction_batch_without_softmax, 1), y_TF_DTYPE), y_batch)\n",
        "        # correct_prediction = tf.Print(correct_prediction, [correct_prediction], \"correct_prediction: \")\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        return net, cost, accuracy, y_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jpISk8SQqCO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Train, validate and test"
      ]
    },
    {
      "metadata": {
        "id": "3iOAaIdbQoQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4865
        },
        "outputId": "3c263b14-7991-4fd8-fcb4-281d5a79a69b"
      },
      "cell_type": "code",
      "source": [
        "# Train, validate and test\n",
        "batch_size = 128\n",
        "n_epoch = 25\n",
        "n_step_per_epoch = int(len(train_set[y]) / batch_size)\n",
        "n_step = n_epoch * n_step_per_epoch\n",
        "print_freq = 1\n",
        "checkpoint_freq = 3\n",
        "learning_rate = 0.0001\n",
        "\n",
        "model_ckpt_file_name = os.path.join(working_directory, \"checkpoint\", \"model-quickdraw-cnn.ckpt\")\n",
        "resume = True  # load model, resume from previous checkpoint?\n",
        "\n",
        "\n",
        "def distort_fn(X, is_train=False):\n",
        "    # print('begin', X.shape, np.min(X), np.max(X))\n",
        "\n",
        "    if is_train == True:\n",
        "        # 1. Randomly flip the image horizontally.\n",
        "        X = tf.image.random_flip_left_right(X)\n",
        "\n",
        "    # X = tf.image.per_image_standardization(X)\n",
        "\n",
        "    # print('after norm', X.shape, np.min(X), np.max(X), np.mean(X))\n",
        "    return X\n",
        "\n",
        "\n",
        "def save_model():\n",
        "    model_type = \"saved-model\"\n",
        "    latest_model_directory = f'{model_type}-{time.strftime(\"%Y%m%d%H%M%S\", time.localtime())}'\n",
        "    saved_model_directory = os.path.join(working_directory, latest_model_directory)\n",
        "    if not os.path.exists(saved_model_directory):\n",
        "        tf.saved_model.simple_save(session, saved_model_directory,\n",
        "                                   inputs={\"X\": X_batch_ph},\n",
        "                                   outputs={\"y_output\": y_prediction_})\n",
        "        dist_directory = os.path.join(\".\", model_type)\n",
        "        if os.path.exists(dist_directory):\n",
        "            os.remove(dist_directory)\n",
        "        os.symlink(saved_model_directory, dist_directory, target_is_directory=True)\n",
        "\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "    session = tf.Session(config=config)\n",
        "\n",
        "    #\n",
        "    # Connect to tfdbg dashboard by ```http://localhost:6006#debugger```\n",
        "    # when the following command is issued.\n",
        "    #\n",
        "    # ```bash\n",
        "    # $ tensorboard --logdir logs --port 6006 --debugger_port 6064\n",
        "    # ```\n",
        "    #\n",
        "    # session = tfdebug.TensorBoardDebugWrapperSession(session, \"albert-mbp.local:6064\")\n",
        "\n",
        "    X_batch_ph = tf.placeholder(dtype=X_TF_DTYPE, shape=[None, image_height, image_width, image_depth], name='X_batch')\n",
        "    y_batch_ph = tf.placeholder(dtype=y_TF_DTYPE, shape=[None], name='y_batch')\n",
        "    # X_batch_ph = tf.placeholder(dtype=X_TF_DTYPE, shape=[batch_size, image_height, image_width, image_depth], name='X')\n",
        "    # y_batch_ph = tf.placeholder(dtype=y_TF_DTYPE, shape=[batch_size], name='y')\n",
        "\n",
        "    def perform_minibatch(run_list, X, y, batch_size, is_train=False):\n",
        "        n_batch, sum_loss, sum_accuracy = 0, 0, 0\n",
        "        for X_batch_a, y_batch_a in tl.iterate.minibatches(X, y, batch_size, shuffle=is_train):\n",
        "            # data augmentation for training\n",
        "            # X_batch_a = tl.prepro.threading_data(X_batch_a, fn=distort_fn, is_train=is_train)\n",
        "\n",
        "            cost, accuracy = 0, 0\n",
        "            if is_train:\n",
        "                _, cost, accuracy = session.run(\n",
        "                    run_list, feed_dict={X_batch_ph: X_batch_a, y_batch_ph: y_batch_a}\n",
        "                )\n",
        "            else:\n",
        "                cost, accuracy = session.run(\n",
        "                    run_list, feed_dict={X_batch_ph: X_batch_a, y_batch_ph: y_batch_a}\n",
        "                )\n",
        "\n",
        "            sum_loss += cost\n",
        "            sum_accuracy += accuracy\n",
        "            n_batch += 1\n",
        "        return n_batch, sum_loss, sum_accuracy\n",
        "\n",
        "\n",
        "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
        "        # Build the model\n",
        "        print(\"### Train Network model ###\")\n",
        "        network_, cost_, accuracy_, y_prediction_ = model_batch_normalization(\n",
        "            X_batch_ph, y_batch_ph, n_category, reuse=None, is_train=True\n",
        "        )\n",
        "        print(\"### Reuse this Train Network model for validation and test ###\")\n",
        "        _, cost_test_, accuracy_test_, y_prediction_test_ = model_batch_normalization(\n",
        "            X_batch_ph, y_batch_ph, n_category, reuse=True, is_train=False\n",
        "        )\n",
        "\n",
        "    # Define the training optimizer\n",
        "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
        "        train_op_ = tf.train.AdamOptimizer(learning_rate).minimize(cost_)\n",
        "\n",
        "    tl.layers.initialize_global_variables(session)\n",
        "\n",
        "    # Attach the graph for TensorBoard writer\n",
        "    # tfboard_file_writer.add_graph(tf.get_default_graph())\n",
        "    tfboard_file_writer.add_graph(session.graph)\n",
        "\n",
        "    if resume and os.path.isfile(model_ckpt_file_name):\n",
        "        print(\"Load existing model \" + \"!\" * 10)\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(session, model_ckpt_file_name)\n",
        "\n",
        "    print(\"### Network parameters ###\")\n",
        "    network_.print_params(False)\n",
        "    print(\"### Network layers ###\")\n",
        "    network_.print_layers()\n",
        "\n",
        "    print('   learning_rate: %f' % learning_rate)\n",
        "    print('   batch_size: %d' % batch_size)\n",
        "    print('   n_epoch: %d, step in an epoch: %d, total n_step: %d' % (n_epoch, n_step_per_epoch, n_step))\n",
        "\n",
        "    step, sum_batch, sum_loss, sum_accuracy = 0, 0, 0, 0\n",
        "    for epoch in range(n_epoch):\n",
        "        start_time = time.time()\n",
        "\n",
        "        n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "            [train_op_, cost_, accuracy_],\n",
        "            train_set[X], train_set[y], batch_size, is_train=True\n",
        "        )\n",
        "        sum_batch += n_batch_a_epoch\n",
        "        sum_loss += cost_a_epoch\n",
        "        sum_accuracy += accuracy_a_epoch\n",
        "        step += n_batch_a_epoch\n",
        "\n",
        "        assert n_batch_a_epoch == n_step_per_epoch\n",
        "\n",
        "        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
        "            print(\"Epoch %d : Step %d-%d of %d took %fs\" %\n",
        "                  (epoch + 1, step - n_step_per_epoch, step, n_step, time.time() - start_time))\n",
        "            print(\"   train loss: %f\" % (sum_loss / sum_batch))\n",
        "            print(\"   train accuracy: %f\" % (sum_accuracy / sum_batch))\n",
        "            sum_batch, sum_loss, sum_accuracy = 0, 0, 0\n",
        "\n",
        "            n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "                [cost_test_, accuracy_test_],\n",
        "                validation_set[X], validation_set[y], batch_size\n",
        "            )\n",
        "            print(\"   validation loss: %f\" % (cost_a_epoch / n_batch_a_epoch))\n",
        "            print(\"   validation accuracy: %f\" % (accuracy_a_epoch / n_batch_a_epoch))\n",
        "\n",
        "            n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "                [cost_test_, accuracy_test_],\n",
        "                test_set[X], test_set[y], batch_size\n",
        "            )\n",
        "            print(\"   test loss: %f\" % (cost_a_epoch / n_batch_a_epoch))\n",
        "            print(\"   test accuracy: %f\" % (accuracy_a_epoch / n_batch_a_epoch))\n",
        "\n",
        "        # Save model when checkpoint\n",
        "        if (epoch + 1) % checkpoint_freq == 0:\n",
        "            print(\"Saving checkpoint... \" + \"!\" * 10)\n",
        "            saver = tf.train.Saver()\n",
        "            save_path = saver.save(session, model_ckpt_file_name)\n",
        "            print(\"Saving model... \" + \"!\" * 10)\n",
        "            save_model()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Train Network model ###\n",
            "[TL] InputLayer  model/input: (?, 28, 28, 1)\n",
            "[TL] Conv2d model/cnn1: n_filter: 128 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 256 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 12544\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (12544, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "### Reuse this Train Network model for validation and test ###\n",
            "[TL] InputLayer  model/input: (?, 28, 28, 1)\n",
            "[TL] Conv2d model/cnn1: n_filter: 128 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 256 filter_size: (5, 5) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 12544\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (12544, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "[TL] WARNING: Function: `tensorlayer.layers.utils.initialize_global_variables` (in file: /usr/local/lib/python3.6/dist-packages/tensorlayer/layers/utils.py) is deprecated and will be removed after 2018-09-30.\n",
            "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
            "\n",
            "### Network parameters ###\n",
            "[TL]   param   0: model/cnn1/kernel:0  (5, 5, 1, 128)     float32_ref\n",
            "[TL]   param   1: model/batch1/beta:0  (128,)             float32_ref\n",
            "[TL]   param   2: model/batch1/gamma:0 (128,)             float32_ref\n",
            "[TL]   param   3: model/batch1/moving_mean:0 (128,)             float32_ref\n",
            "[TL]   param   4: model/batch1/moving_variance:0 (128,)             float32_ref\n",
            "[TL]   param   5: model/cnn2/kernel:0  (5, 5, 128, 256)    float32_ref\n",
            "[TL]   param   6: model/batch2/beta:0  (256,)             float32_ref\n",
            "[TL]   param   7: model/batch2/gamma:0 (256,)             float32_ref\n",
            "[TL]   param   8: model/batch2/moving_mean:0 (256,)             float32_ref\n",
            "[TL]   param   9: model/batch2/moving_variance:0 (256,)             float32_ref\n",
            "[TL]   param  10: model/d1relu/W:0     (12544, 384)       float32_ref\n",
            "[TL]   param  11: model/d1relu/b:0     (384,)             float32_ref\n",
            "[TL]   param  12: model/d2relu/W:0     (384, 192)         float32_ref\n",
            "[TL]   param  13: model/d2relu/b:0     (192,)             float32_ref\n",
            "[TL]   param  14: model/output/W:0     (192, 10)          float32_ref\n",
            "[TL]   param  15: model/output/b:0     (10,)              float32_ref\n",
            "[TL]   num of params: 5717066\n",
            "### Network layers ###\n",
            "[TL]   layer   0: X_batch:0            (?, 28, 28, 1)     float32\n",
            "[TL]   layer   1: model/cnn1/Conv2D:0  (?, 28, 28, 128)    float32\n",
            "[TL]   layer   2: model/batch1/Relu:0  (?, 28, 28, 128)    float32\n",
            "[TL]   layer   3: model/pool1/MaxPool:0 (?, 14, 14, 128)    float32\n",
            "[TL]   layer   4: model/cnn2/Conv2D:0  (?, 14, 14, 256)    float32\n",
            "[TL]   layer   5: model/batch2/Relu:0  (?, 14, 14, 256)    float32\n",
            "[TL]   layer   6: model/pool2/MaxPool:0 (?, 7, 7, 256)     float32\n",
            "[TL]   layer   7: model/flatten:0      (?, 12544)         float32\n",
            "[TL]   layer   8: model/d1relu/Relu:0  (?, 384)           float32\n",
            "[TL]   layer   9: model/d2relu/Relu:0  (?, 192)           float32\n",
            "[TL]   layer  10: model/output/bias_add:0 (?, 10)            float32\n",
            "   learning_rate: 0.000100\n",
            "   batch_size: 128\n",
            "   n_epoch: 25, step in an epoch: 7812, total n_step: 195300\n",
            "Epoch 1 : Step 0-7812 of 195300 took 361.912277s\n",
            "   train loss: 1.257813\n",
            "   train accuracy: 0.905219\n",
            "   validation loss: 0.321441\n",
            "   validation accuracy: 0.927374\n",
            "   test loss: 0.367277\n",
            "   test accuracy: 0.911490\n",
            "Epoch 2 : Step 7812-15624 of 195300 took 375.490463s\n",
            "   train loss: 0.263772\n",
            "   train accuracy: 0.938772\n",
            "   validation loss: 0.234744\n",
            "   validation accuracy: 0.944863\n",
            "   test loss: 0.266478\n",
            "   test accuracy: 0.935145\n",
            "Epoch 3 : Step 15624-23436 of 195300 took 364.274757s\n",
            "   train loss: 0.224582\n",
            "   train accuracy: 0.945857\n",
            "   validation loss: 0.220916\n",
            "   validation accuracy: 0.945357\n",
            "   test loss: 0.258242\n",
            "   test accuracy: 0.931443\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Pass your op to the equivalent parameter main_op instead.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181016044810/saved_model.pb\n",
            "Epoch 4 : Step 23436-31248 of 195300 took 361.570598s\n",
            "   train loss: 0.205480\n",
            "   train accuracy: 0.949367\n",
            "   validation loss: 0.201696\n",
            "   validation accuracy: 0.949860\n",
            "   test loss: 0.215675\n",
            "   test accuracy: 0.944537\n",
            "Epoch 5 : Step 31248-39060 of 195300 took 372.039419s\n",
            "   train loss: 0.193394\n",
            "   train accuracy: 0.952017\n",
            "   validation loss: 0.193659\n",
            "   validation accuracy: 0.951412\n",
            "   test loss: 0.202539\n",
            "   test accuracy: 0.947790\n",
            "Epoch 6 : Step 39060-46872 of 195300 took 364.147208s\n",
            "   train loss: 0.184442\n",
            "   train accuracy: 0.953895\n",
            "   validation loss: 0.199602\n",
            "   validation accuracy: 0.948705\n",
            "   test loss: 0.225472\n",
            "   test accuracy: 0.939855\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181016050902/saved_model.pb\n",
            "Epoch 7 : Step 46872-54684 of 195300 took 362.631638s\n",
            "   train loss: 0.177834\n",
            "   train accuracy: 0.955379\n",
            "   validation loss: 0.186369\n",
            "   validation accuracy: 0.952443\n",
            "   test loss: 0.211462\n",
            "   test accuracy: 0.943366\n",
            "Epoch 8 : Step 54684-62496 of 195300 took 365.265229s\n",
            "   train loss: 0.172295\n",
            "   train accuracy: 0.956711\n",
            "   validation loss: 0.185596\n",
            "   validation accuracy: 0.952237\n",
            "   test loss: 0.213897\n",
            "   test accuracy: 0.942223\n",
            "Epoch 9 : Step 62496-70308 of 195300 took 365.043230s\n",
            "   train loss: 0.167702\n",
            "   train accuracy: 0.957917\n",
            "   validation loss: 0.180536\n",
            "   validation accuracy: 0.953631\n",
            "   test loss: 0.204072\n",
            "   test accuracy: 0.945340\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181016052947/saved_model.pb\n",
            "Epoch 10 : Step 70308-78120 of 195300 took 368.504460s\n",
            "   train loss: 0.163619\n",
            "   train accuracy: 0.958793\n",
            "   validation loss: 0.180560\n",
            "   validation accuracy: 0.953110\n",
            "   test loss: 0.210584\n",
            "   test accuracy: 0.943285\n",
            "Epoch 11 : Step 78120-85932 of 195300 took 367.895751s\n",
            "   train loss: 0.160336\n",
            "   train accuracy: 0.959718\n",
            "   validation loss: 0.180645\n",
            "   validation accuracy: 0.953200\n",
            "   test loss: 0.207812\n",
            "   test accuracy: 0.943666\n",
            "Epoch 12 : Step 85932-93744 of 195300 took 361.798360s\n",
            "   train loss: 0.157531\n",
            "   train accuracy: 0.960404\n",
            "   validation loss: 0.176896\n",
            "   validation accuracy: 0.954122\n",
            "   test loss: 0.189497\n",
            "   test accuracy: 0.949396\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181016055037/saved_model.pb\n",
            "Epoch 13 : Step 93744-101556 of 195300 took 369.321077s\n",
            "   train loss: 0.154804\n",
            "   train accuracy: 0.961101\n",
            "   validation loss: 0.179208\n",
            "   validation accuracy: 0.953646\n",
            "   test loss: 0.200788\n",
            "   test accuracy: 0.946388\n",
            "Epoch 14 : Step 101556-109368 of 195300 took 356.569286s\n",
            "   train loss: 0.152362\n",
            "   train accuracy: 0.961925\n",
            "   validation loss: 0.175366\n",
            "   validation accuracy: 0.954763\n",
            "   test loss: 0.201252\n",
            "   test accuracy: 0.946102\n",
            "Epoch 15 : Step 109368-117180 of 195300 took 369.918437s\n",
            "   train loss: 0.149791\n",
            "   train accuracy: 0.962751\n",
            "   validation loss: 0.174567\n",
            "   validation accuracy: 0.955318\n",
            "   test loss: 0.196643\n",
            "   test accuracy: 0.946959\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181016061126/saved_model.pb\n",
            "Epoch 16 : Step 117180-124992 of 195300 took 360.878420s\n",
            "   train loss: 0.147654\n",
            "   train accuracy: 0.963294\n",
            "   validation loss: 0.175685\n",
            "   validation accuracy: 0.954636\n",
            "   test loss: 0.192131\n",
            "   test accuracy: 0.948320\n",
            "Epoch 17 : Step 124992-132804 of 195300 took 356.786852s\n",
            "   train loss: 0.145495\n",
            "   train accuracy: 0.963951\n",
            "   validation loss: 0.181317\n",
            "   validation accuracy: 0.953301\n",
            "   test loss: 0.210191\n",
            "   test accuracy: 0.943815\n",
            "Epoch 18 : Step 132804-140616 of 195300 took 362.179748s\n",
            "   train loss: 0.143396\n",
            "   train accuracy: 0.964511\n",
            "   validation loss: 0.175390\n",
            "   validation accuracy: 0.954842\n",
            "   test loss: 0.182844\n",
            "   test accuracy: 0.952118\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181016063201/saved_model.pb\n",
            "Epoch 19 : Step 140616-148428 of 195300 took 367.175720s\n",
            "   train loss: 0.141427\n",
            "   train accuracy: 0.965036\n",
            "   validation loss: 0.173760\n",
            "   validation accuracy: 0.955479\n",
            "   test loss: 0.195087\n",
            "   test accuracy: 0.948212\n",
            "Epoch 20 : Step 148428-156240 of 195300 took 361.873354s\n",
            "   train loss: 0.139606\n",
            "   train accuracy: 0.965712\n",
            "   validation loss: 0.175486\n",
            "   validation accuracy: 0.955044\n",
            "   test loss: 0.193993\n",
            "   test accuracy: 0.948239\n",
            "Epoch 21 : Step 156240-164052 of 195300 took 366.963988s\n",
            "   train loss: 0.137936\n",
            "   train accuracy: 0.966144\n",
            "   validation loss: 0.177880\n",
            "   validation accuracy: 0.954696\n",
            "   test loss: 0.195520\n",
            "   test accuracy: 0.947871\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181016065248/saved_model.pb\n",
            "Epoch 22 : Step 164052-171864 of 195300 took 370.025730s\n",
            "   train loss: 0.135894\n",
            "   train accuracy: 0.966827\n",
            "   validation loss: 0.177177\n",
            "   validation accuracy: 0.954816\n",
            "   test loss: 0.194997\n",
            "   test accuracy: 0.948919\n",
            "Epoch 23 : Step 171864-179676 of 195300 took 369.672494s\n",
            "   train loss: 0.134058\n",
            "   train accuracy: 0.967347\n",
            "   validation loss: 0.178714\n",
            "   validation accuracy: 0.954178\n",
            "   test loss: 0.202996\n",
            "   test accuracy: 0.946347\n",
            "Epoch 24 : Step 179676-187488 of 195300 took 366.422915s\n",
            "   train loss: 0.132612\n",
            "   train accuracy: 0.967802\n",
            "   validation loss: 0.181521\n",
            "   validation accuracy: 0.954141\n",
            "   test loss: 0.185986\n",
            "   test accuracy: 0.951328\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181016071344/saved_model.pb\n",
            "Epoch 25 : Step 187488-195300 of 195300 took 362.844779s\n",
            "   train loss: 0.130931\n",
            "   train accuracy: 0.968412\n",
            "   validation loss: 0.180198\n",
            "   validation accuracy: 0.954430\n",
            "   test loss: 0.197798\n",
            "   test accuracy: 0.948402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UbFcV5edSMlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save the trained model"
      ]
    },
    {
      "metadata": {
        "id": "4wFMK2v6SMvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4ac7e1ef-a7c9-4fce-9809-c680106e498c"
      },
      "cell_type": "code",
      "source": [
        "save_model()\n",
        "\n",
        "tfboard_file_writer.flush()\n",
        "tfboard_file_writer.close()\n",
        "\n",
        "session.close()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181016074029/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3e3lOIaaTd0s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert to TensorFlow.js web model"
      ]
    },
    {
      "metadata": {
        "id": "CrdV5utoTeB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1074
        },
        "outputId": "386e73c0-3239-44e4-afcd-071af77e89d1"
      },
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=tf_saved_model --output_node_names=\"model/y_output\" saved-model web-model\n",
        "!echo \"Current directory ->\"\n",
        "!ls -la\n",
        "!echo \"web-model directory ->\"\n",
        "!ls -la web-model\n",
        "!echo \"saved-model directory ->\"\n",
        "!ls -la saved-model"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2018-10-16 07:40:46.982535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2018-10-16 07:40:46.983029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 8.92GiB\n",
            "2018-10-16 07:40:46.983090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
            "2018-10-16 07:40:47.443933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-10-16 07:40:47.444010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
            "2018-10-16 07:40:47.444065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
            "2018-10-16 07:40:47.444423: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2018-10-16 07:40:47.444518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8629 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2018-10-16 07:40:48.938502: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: graph_to_optimize\n",
            "2018-10-16 07:40:48.938601: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   debug_stripper: Graph size after: 66 nodes (0), 67 edges (0), time = 0.079ms.\n",
            "2018-10-16 07:40:48.938635: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   model_pruner: Graph size after: 50 nodes (-16), 51 edges (-16), time = 0.349ms.\n",
            "2018-10-16 07:40:48.938681: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 30 nodes (-20), 29 edges (-22), time = 99.402ms.\n",
            "2018-10-16 07:40:48.938712: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   arithmetic_optimizer: Graph size after: 30 nodes (0), 29 edges (0), time = 65.574ms.\n",
            "2018-10-16 07:40:48.938738: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   dependency_optimizer: Graph size after: 26 nodes (-4), 25 edges (-4), time = 1.624ms.\n",
            "2018-10-16 07:40:48.938764: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   model_pruner: Graph size after: 26 nodes (0), 25 edges (0), time = 0.751ms.\n",
            "2018-10-16 07:40:48.938789: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 26 nodes (0), 25 edges (0), time = 28.424ms.\n",
            "2018-10-16 07:40:48.938814: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   arithmetic_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 65.797ms.\n",
            "2018-10-16 07:40:48.938840: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   dependency_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 1.94ms.\n",
            "2018-10-16 07:40:48.938865: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   debug_stripper: Graph size after: 26 nodes (0), 25 edges (0), time = 0.754ms.\n",
            "2018-10-16 07:40:48.938890: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   model_pruner: Graph size after: 26 nodes (0), 25 edges (0), time = 0.784ms.\n",
            "2018-10-16 07:40:48.938917: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 26 nodes (0), 25 edges (0), time = 26.89ms.\n",
            "2018-10-16 07:40:48.938945: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   arithmetic_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 48.662ms.\n",
            "2018-10-16 07:40:48.938977: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   dependency_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 1.818ms.\n",
            "2018-10-16 07:40:48.939008: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   model_pruner: Graph size after: 26 nodes (0), 25 edges (0), time = 0.735ms.\n",
            "2018-10-16 07:40:48.939040: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 26 nodes (0), 25 edges (0), time = 33.837ms.\n",
            "2018-10-16 07:40:48.939082: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   arithmetic_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 48.798ms.\n",
            "2018-10-16 07:40:48.939116: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   dependency_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 1.748ms.\n",
            "Writing weight file web-model/tensorflowjs_model.pb...\n",
            "2018-10-16 07:40:48.999919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
            "2018-10-16 07:40:49.000017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-10-16 07:40:49.000053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
            "2018-10-16 07:40:49.000101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
            "2018-10-16 07:40:49.000398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8629 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Current directory ->\n",
            "total 28\n",
            "drwxr-xr-x  1 root root 4096 Oct 16 07:40 .\n",
            "drwxr-xr-x  1 root root 4096 Oct 16 04:11 ..\n",
            "drwxr-xr-x  4 root root 4096 Oct 13 01:38 .config\n",
            "drwxr-xr-x 13 root root 4096 Oct 16 07:40 data\n",
            "drwxr-xr-x  2 root root 4096 Oct 16 04:27 logs\n",
            "drwxr-xr-x  2 root root 4096 Oct 13 01:58 sample_data\n",
            "lrwxrwxrwx  1 root root   31 Oct 16 07:40 saved-model -> data/saved-model-20181016074029\n",
            "drwxr-xr-x  2 root root 4096 Oct 16 07:40 web-model\n",
            "web-model directory ->\n",
            "total 25556\n",
            "drwxr-xr-x 2 root root    4096 Oct 16 07:40 .\n",
            "drwxr-xr-x 1 root root    4096 Oct 16 07:40 ..\n",
            "-rw-r--r-- 1 root root 4194304 Oct 16 07:40 group1-shard1of6\n",
            "-rw-r--r-- 1 root root 4194304 Oct 16 07:40 group1-shard2of6\n",
            "-rw-r--r-- 1 root root 4194304 Oct 16 07:40 group1-shard3of6\n",
            "-rw-r--r-- 1 root root 4194304 Oct 16 07:40 group1-shard4of6\n",
            "-rw-r--r-- 1 root root 4194304 Oct 16 07:40 group1-shard5of6\n",
            "-rw-r--r-- 1 root root 1890608 Oct 16 07:40 group1-shard6of6\n",
            "-rw-r--r-- 1 root root 3292061 Oct 16 07:40 tensorflowjs_model.pb\n",
            "-rw-r--r-- 1 root root     790 Oct 16 07:40 weights_manifest.json\n",
            "saved-model directory ->\n",
            "lrwxrwxrwx 1 root root 31 Oct 16 07:40 saved-model -> data/saved-model-20181016074029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q8g244v4VZUj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Zip and download the models"
      ]
    },
    {
      "metadata": {
        "id": "0fzcYLP3VB97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "42cbbc5d-c460-4316-b640-d06bcbab47fe"
      },
      "cell_type": "code",
      "source": [
        "!zip -r web-model.zip web-model\n",
        "!zip -r saved-model.zip saved-model\n",
        "!ls -la *.zip\n",
        "\n",
        "from google.colab import files\n",
        "files.download('web-model.zip')\n",
        "files.download('saved-model.zip')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: web-model/ (stored 0%)\n",
            "  adding: web-model/weights_manifest.json (deflated 72%)\n",
            "  adding: web-model/group1-shard6of6 (deflated 8%)\n",
            "  adding: web-model/group1-shard5of6 (deflated 7%)\n",
            "  adding: web-model/group1-shard1of6 (deflated 7%)\n",
            "  adding: web-model/group1-shard4of6 (deflated 7%)\n",
            "  adding: web-model/tensorflowjs_model.pb (deflated 7%)\n",
            "  adding: web-model/group1-shard2of6 (deflated 7%)\n",
            "  adding: web-model/group1-shard3of6 (deflated 7%)\n",
            "  adding: saved-model/ (stored 0%)\n",
            "  adding: saved-model/variables/ (stored 0%)\n",
            "  adding: saved-model/variables/variables.data-00000-of-00001 (deflated 30%)\n",
            "  adding: saved-model/variables/variables.index (deflated 46%)\n",
            "  adding: saved-model/saved_model.pb (deflated 91%)\n",
            "-rw-r--r-- 1 root root 48361888 Oct 16 07:42 saved-model.zip\n",
            "-rw-r--r-- 1 root root 24228564 Oct 16 07:41 web-model.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 35264, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 721, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 800, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}