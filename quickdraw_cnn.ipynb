{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickdraw-cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/AlbertZheng/quickdraw-cnn/blob/master/quickdraw_cnn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "qs0seV43MwNv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## \"You draw, I guess.\" is a MVP (Minimium Viable Product) that uses CNN to recognize the sketch drawings on web canvas.\n",
        "### The CNN was trained to recognize 10 classes using <a href='https://github.com/googlecreativelab/quickdraw-dataset'>\"The Quick, Draw! Dataset\" </a> of Google awesome \"猜画小歌\" Wechat App."
      ]
    },
    {
      "metadata": {
        "id": "0_FT6bGsOY22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install dependent packages"
      ]
    },
    {
      "metadata": {
        "id": "EDJoFej9MZ83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1244
        },
        "outputId": "fc25d7fb-46c0-4aad-da52-cc720be9c76b"
      },
      "cell_type": "code",
      "source": [
        "!pip install \"tensorlayer>=1.10\"\n",
        "!pip install tensorflowjs\n",
        "!pip list|grep tensor"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorlayer>=1.10 in /usr/local/lib/python3.6/dist-packages (1.10.1)\n",
            "Requirement already satisfied: scipy<1.2,>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (0.19.2)\n",
            "Requirement already satisfied: imageio<2.5,>=2.3 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (2.4.1)\n",
            "Requirement already satisfied: wrapt<1.11,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (1.10.11)\n",
            "Requirement already satisfied: progressbar2<3.39,>=3.38 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (3.38.0)\n",
            "Requirement already satisfied: matplotlib<2.3,>=2.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (2.2.3)\n",
            "Requirement already satisfied: requests<2.20,>=2.19 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (2.19.1)\n",
            "Requirement already satisfied: tqdm<4.26,>=4.23 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (4.25.0)\n",
            "Requirement already satisfied: numpy<1.16,>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (1.15.1)\n",
            "Requirement already satisfied: scikit-image<0.15,>=0.14 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (0.14.1)\n",
            "Requirement already satisfied: lxml<4.3,>=4.2 in /usr/local/lib/python3.6/dist-packages (from tensorlayer>=1.10) (4.2.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<2.5,>=2.3->tensorlayer>=1.10) (5.3.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer>=1.10) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2<3.39,>=3.38->tensorlayer>=1.10) (1.11.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2.2.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (2018.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<2.3,>=2.2->tensorlayer>=1.10) (1.0.1)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (2018.8.24)\n",
            "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.20,>=2.19->tensorlayer>=1.10) (1.22)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (1.0.1)\n",
            "Requirement already satisfied: dask[array]>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (0.19.3)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (0.5.6)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15,>=0.14->tensorlayer>=1.10) (2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib<2.3,>=2.2->tensorlayer>=1.10) (39.1.0)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=0.9.0->scikit-image<0.15,>=0.14->tensorlayer>=1.10) (0.9.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image<0.15,>=0.14->tensorlayer>=1.10) (4.3.0)\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: six==1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.11.0)\n",
            "Requirement already satisfied: tensorflow-hub==0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.1.1)\n",
            "Requirement already satisfied: keras==2.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.2.2)\n",
            "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: numpy==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.15.1)\n",
            "Requirement already satisfied: tensorflow==1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.11.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.1.1->tensorflowjs) (3.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2->tensorflowjs) (3.13)\n",
            "Collecting keras-preprocessing==1.0.2 (from keras==2.2.2->tensorflowjs)\n",
            "  Downloading https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2->tensorflowjs) (1.1.0)\n",
            "Collecting keras-applications==1.0.4 (from keras==2.2.2->tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (0.31.1)\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (39.1.0)\n",
            "Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (1.11.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (0.5.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0->tensorflowjs) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0->tensorflowjs) (0.14.1)\n",
            "\u001b[31mtensorflow 1.11.0 has requirement keras-applications>=1.0.5, but you'll have keras-applications 1.0.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorflow 1.11.0 has requirement keras-preprocessing>=1.0.3, but you'll have keras-preprocessing 1.0.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-preprocessing, keras-applications\n",
            "  Found existing installation: Keras-Preprocessing 1.0.5\n",
            "    Uninstalling Keras-Preprocessing-1.0.5:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.0.5\n",
            "  Found existing installation: Keras-Applications 1.0.6\n",
            "    Uninstalling Keras-Applications-1.0.6:\n",
            "      Successfully uninstalled Keras-Applications-1.0.6\n",
            "Successfully installed keras-applications-1.0.4 keras-preprocessing-1.0.2\n",
            "tensorboard              1.11.0   \n",
            "tensorflow               1.11.0   \n",
            "tensorflow-hub           0.1.1    \n",
            "tensorflowjs             0.6.2    \n",
            "tensorlayer              1.10.1   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QV6AypOzOiws",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dependences, and check if GPU is available"
      ]
    },
    {
      "metadata": {
        "id": "W4-1-Q-EOi8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "532bf80c-50aa-4bc8-873a-47aabad5fb15"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "from random import randint\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import *\n",
        "from tensorflow.python import debug as tfdebug\n",
        "\n",
        "\"\"\" Notice to put ```import matplotlib.pyplot``` after imports of tensorlayer, \n",
        "otherwise you will get below warning:\n",
        "\n",
        "This call to matplotlib.use() has no effect because the backend has already\n",
        "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
        "or matplotlib.backends is imported for the first time.\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('### device name: {} ###'.format(device_name))\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('*** GPU device not found ***')\n",
        "print('### Found GPU at: {} ###'.format(device_name))\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### device name: /device:GPU:0 ###\n",
            "### Found GPU at: /device:GPU:0 ###\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q27izcVnPcQu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the Quick, Draw dataset"
      ]
    },
    {
      "metadata": {
        "id": "QeDBh_7WPc0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1564
        },
        "outputId": "6688c25e-0d8c-4c52-fed0-386051a172cb"
      },
      "cell_type": "code",
      "source": [
        "working_directory = 'data'\n",
        "dataset_directory = 'data/quickdraw'\n",
        "# categories_filename = 'categories.txt'\n",
        "# categories_file_url_source = 'https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/'\n",
        "\n",
        "# npy_dataset_url_source = 'https://storage.cloud.google.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "npy_dataset_url_source = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "\n",
        "X = \"X\"\n",
        "y = \"y\"\n",
        "\n",
        "X_NUMPY_DTYPE = np.float32\n",
        "y_NUMPY_DTYPE = np.int64\n",
        "X_TF_DTYPE = tf.float32\n",
        "y_TF_DTYPE = tf.int32\n",
        "\n",
        "image_height = 28\n",
        "image_width = 28\n",
        "image_depth = 1\n",
        "image_size = image_height * image_width * image_depth\n",
        "input_layer_X_shape = [image_height, image_width, image_depth]\n",
        "input_layer_X_shape_batch = [-1, image_height, image_width, image_depth]\n",
        "\n",
        "mini_categories_file_url_source = 'https://raw.githubusercontent.com/AlbertZheng/quickdraw-cnn/master/web/'\n",
        "mini_categories_filename = 'mini-categories.txt'\n",
        "n_category = 10  # The maximum of category number is up to 345\n",
        "n_train_example_per_category = 50000\n",
        "\n",
        "\n",
        "def print_dataset_shape(X_name, X, y_name, y):\n",
        "    print(X_name + '.shape ', X.shape, end='\\t,\\t')\n",
        "    print(y_name + '.shape ', y.shape)\n",
        "    print('%s.dtype %s\\t,\\t%s.dtype %s' % (X_name, X.dtype, y_name, y.dtype))\n",
        "\n",
        "\n",
        "def show_image(X, y, categories):\n",
        "    plt.imshow(X.reshape(image_height, image_width), cmap=\"gray\", interpolation='nearest')\n",
        "    plt.title(f\"{categories[y]}(label: {y})\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_quickdraw_dataset(\n",
        "    n_category=10, n_train_example_per_category=20000\n",
        "):\n",
        "    \"\"\" Download the quick draw data set. \"\"\"\n",
        "    n_validation_example_per_category = int(n_train_example_per_category / 0.7 * 0.2)\n",
        "    n_test_example_per_category = int(n_train_example_per_category / 0.7 * 0.1)\n",
        "\n",
        "    # Download the categories file\n",
        "    tl.files.utils.maybe_download_and_extract(mini_categories_filename, dataset_directory, mini_categories_file_url_source)\n",
        "\n",
        "    tl.logging.info(\"Load or Download quick draw > {}\".format(dataset_directory))\n",
        "\n",
        "    train_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "    validation_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "    test_set = {X: np.empty([0, image_size], dtype=X_NUMPY_DTYPE), y: np.empty([0], dtype=y_NUMPY_DTYPE)}\n",
        "\n",
        "    category_names = [line.rstrip('\\n') for line in open(f\"{dataset_directory}/{mini_categories_filename}\")]\n",
        "    for category_index, category_name in enumerate(category_names):\n",
        "        if category_index == n_category:\n",
        "            break\n",
        "\n",
        "        category_names[category_index], _, _ = category_name.rpartition('=')\n",
        "        category_name = category_names[category_index]\n",
        "\n",
        "        filename = urllib.parse.quote(category_name) + '.npy'\n",
        "        tl.files.utils.maybe_download_and_extract(filename, dataset_directory, npy_dataset_url_source)\n",
        "\n",
        "        data = np.load(os.path.join(dataset_directory, filename))\n",
        "        size_per_category = data.shape[0]\n",
        "        labels = np.full(size_per_category, category_index)\n",
        "\n",
        "        print(f\"### Category '{category_name}' id:{category_index} dataset info ###\")\n",
        "        print_dataset_shape(\"data\", data, \"labels\", labels)\n",
        "\n",
        "        number_begin = 0\n",
        "        number_end = n_train_example_per_category\n",
        "        # train_set[X] = np.concatenate((train_set[X], data[number_begin: number_end, :]), axis=0)\n",
        "        train_set[X] = np.vstack((train_set[X], data[number_begin: number_end, :]))\n",
        "        train_set[y] = np.append(train_set[y], labels[number_begin: number_end])\n",
        "\n",
        "        number_begin += n_train_example_per_category\n",
        "        number_end += n_validation_example_per_category\n",
        "        # validation_set[X] = np.concatenate((validation_set[X], data[number_begin:number_end, :]), axis=0)\n",
        "        validation_set[X] = np.vstack((validation_set[X], data[number_begin:number_end, :]))\n",
        "        validation_set[y] = np.append(validation_set[y], labels[number_begin:number_end])\n",
        "\n",
        "        number_begin += n_validation_example_per_category\n",
        "        number_end += n_test_example_per_category\n",
        "        # test_set[X] = np.concatenate((test_set[X], data[number_begin:number_end, :]), axis=0)\n",
        "        test_set[X] = np.vstack((test_set[X], data[number_begin:number_end, :]))\n",
        "        test_set[y] = np.append(test_set[y], labels[number_begin:number_end])\n",
        "\n",
        "        print_dataset_shape(\"train_set[X]\", train_set[X], \"train_set[y]\", train_set[y])\n",
        "        print_dataset_shape(\"validation_set[X]\", validation_set[X], \"validation_set[y]\", validation_set[y])\n",
        "        print_dataset_shape(\"test_set[X]\", test_set[X], \"test_set[y]\", test_set[y])\n",
        "\n",
        "    # Randomize the dataset\n",
        "    size_per_set = train_set[X].shape[0]\n",
        "    permutation = np.random.permutation(size_per_set)\n",
        "    train_set[X] = train_set[X][permutation, :]\n",
        "    train_set[y] = train_set[y][permutation]\n",
        "\n",
        "    size_per_set = validation_set[X].shape[0]\n",
        "    permutation = np.random.permutation(size_per_set)\n",
        "    validation_set[X] = validation_set[X][permutation, :]\n",
        "    validation_set[y] = validation_set[y][permutation]\n",
        "\n",
        "    size_per_set = test_set[X].shape[0]\n",
        "    permutation = np.random.permutation(size_per_set)\n",
        "    test_set[X] = test_set[X][permutation, :]\n",
        "    test_set[y] = test_set[y][permutation]\n",
        "\n",
        "    # Reshape for CNN input\n",
        "    train_set[X] = train_set[X].reshape(input_layer_X_shape_batch)\n",
        "    validation_set[X] = validation_set[X].reshape(input_layer_X_shape_batch)\n",
        "    test_set[X] = test_set[X].reshape(input_layer_X_shape_batch)\n",
        "\n",
        "    # The original grayscale image is 'black background (x==0) and gray~white (0< x <=255) brush'\n",
        "    # Because the CNN model doesn't need to learn the grayscale values and it only needs to\n",
        "    # learn the strokes, we normalize it to 'white background (x==1) and block (x==0) brush'.\n",
        "    train_set[X] = 1.0 - np.ceil(train_set[X] / 255.0)\n",
        "    validation_set[X] = 1.0 - np.ceil(validation_set[X] / 255.0)\n",
        "    test_set[X] = 1.0 - np.ceil(test_set[X] / 255.0)\n",
        "\n",
        "    return category_names, train_set, validation_set, test_set\n",
        "\n",
        "\n",
        "# Open TensorBoard logs writer\n",
        "tfboard_file_writer = tf.summary.FileWriter('logs')\n",
        "\n",
        "# Download data\n",
        "category_names, train_set, validation_set, test_set = load_quickdraw_dataset(n_category, n_train_example_per_category)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TL] Load or Download quick draw > data/quickdraw\n",
            "### Category 'airplane' id:0 dataset info ###\n",
            "data.shape  (151623, 784)\t,\tlabels.shape  (151623,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (50000, 784)\t,\ttrain_set[y].shape  (50000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (14285, 784)\t,\tvalidation_set[y].shape  (14285,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (7142, 784)\t,\ttest_set[y].shape  (7142,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'alarm clock' id:1 dataset info ###\n",
            "data.shape  (123399, 784)\t,\tlabels.shape  (123399,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (100000, 784)\t,\ttrain_set[y].shape  (100000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (28570, 784)\t,\tvalidation_set[y].shape  (28570,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (14284, 784)\t,\ttest_set[y].shape  (14284,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'apple' id:2 dataset info ###\n",
            "data.shape  (144722, 784)\t,\tlabels.shape  (144722,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (150000, 784)\t,\ttrain_set[y].shape  (150000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (42855, 784)\t,\tvalidation_set[y].shape  (42855,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (21426, 784)\t,\ttest_set[y].shape  (21426,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'bicycle' id:3 dataset info ###\n",
            "data.shape  (126527, 784)\t,\tlabels.shape  (126527,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (200000, 784)\t,\ttrain_set[y].shape  (200000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (57140, 784)\t,\tvalidation_set[y].shape  (57140,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (28568, 784)\t,\ttest_set[y].shape  (28568,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'car' id:4 dataset info ###\n",
            "data.shape  (182764, 784)\t,\tlabels.shape  (182764,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (250000, 784)\t,\ttrain_set[y].shape  (250000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (71425, 784)\t,\tvalidation_set[y].shape  (71425,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (35710, 784)\t,\ttest_set[y].shape  (35710,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'cloud' id:5 dataset info ###\n",
            "data.shape  (120265, 784)\t,\tlabels.shape  (120265,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (300000, 784)\t,\ttrain_set[y].shape  (300000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (85710, 784)\t,\tvalidation_set[y].shape  (85710,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (42852, 784)\t,\ttest_set[y].shape  (42852,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'cup' id:6 dataset info ###\n",
            "data.shape  (130721, 784)\t,\tlabels.shape  (130721,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (350000, 784)\t,\ttrain_set[y].shape  (350000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (99995, 784)\t,\tvalidation_set[y].shape  (99995,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (49994, 784)\t,\ttest_set[y].shape  (49994,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'dog' id:7 dataset info ###\n",
            "data.shape  (152159, 784)\t,\tlabels.shape  (152159,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (400000, 784)\t,\ttrain_set[y].shape  (400000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (114280, 784)\t,\tvalidation_set[y].shape  (114280,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (57136, 784)\t,\ttest_set[y].shape  (57136,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 'eyeglasses' id:8 dataset info ###\n",
            "data.shape  (225762, 784)\t,\tlabels.shape  (225762,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (450000, 784)\t,\ttrain_set[y].shape  (450000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (128565, 784)\t,\tvalidation_set[y].shape  (128565,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (64278, 784)\t,\ttest_set[y].shape  (64278,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n",
            "### Category 't-shirt' id:9 dataset info ###\n",
            "data.shape  (125233, 784)\t,\tlabels.shape  (125233,)\n",
            "data.dtype uint8\t,\tlabels.dtype int64\n",
            "train_set[X].shape  (500000, 784)\t,\ttrain_set[y].shape  (500000,)\n",
            "train_set[X].dtype float32\t,\ttrain_set[y].dtype int64\n",
            "validation_set[X].shape  (142850, 784)\t,\tvalidation_set[y].shape  (142850,)\n",
            "validation_set[X].dtype float32\t,\tvalidation_set[y].dtype int64\n",
            "test_set[X].shape  (71420, 784)\t,\ttest_set[y].shape  (71420,)\n",
            "test_set[X].dtype float32\t,\ttest_set[y].dtype int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LejFWzHKP2Dg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Function: Network model definition"
      ]
    },
    {
      "metadata": {
        "id": "d5TwN6-sP1eQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_batch_normalization(X_batch, y_batch, output_units, reuse, is_train):\n",
        "    \"\"\" Define the network model \"\"\"\n",
        "    W_init1 = tf.truncated_normal_initializer(stddev=5e-2)\n",
        "    W_init2 = tf.truncated_normal_initializer(stddev=0.04)\n",
        "    bias_init = tf.constant_initializer(value=0.1)\n",
        "\n",
        "    with tf.variable_scope(\"model\", reuse=reuse):\n",
        "        net = InputLayer(X_batch, name='input')\n",
        "        net = Conv2d(net, 64, (3, 3), (1, 1), padding='SAME',\n",
        "                     W_init=W_init1, b_init=None, name='cnn1')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch1')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool1')\n",
        "\n",
        "        net = Conv2d(net, 64, (3, 3), (1, 1), padding='SAME',\n",
        "                     W_init=W_init1, b_init=None, name='cnn2')\n",
        "        net = BatchNormLayer(net, is_train, act=tf.nn.relu, name='batch2')\n",
        "        net = MaxPool2d(net, (3, 3), (2, 2), padding='SAME', name='pool2')\n",
        "\n",
        "        net = FlattenLayer(net, name='flatten')\n",
        "        net = DenseLayer(net, 384, act=tf.nn.relu,\n",
        "                         W_init=W_init2, b_init=bias_init, name='d1relu')\n",
        "        net = DenseLayer(net, 192, act=tf.nn.relu,\n",
        "                         W_init=W_init2, b_init=bias_init, name='d2relu')\n",
        "        # The softmax() is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
        "        # speed up computation, so we use identity here.\n",
        "        # see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
        "        net = DenseLayer(net, n_units=output_units, act=None,\n",
        "                         W_init=W_init2, name='output')\n",
        "\n",
        "        y_prediction_batch_without_softmax = net.outputs\n",
        "\n",
        "        # For inference by using this model\n",
        "        # y_output = tf.argmax(tf.nn.softmax(y_prediction_batch_without_softmax), 1)\n",
        "        y_output = tf.nn.softmax(y_prediction_batch_without_softmax, name=\"y_output\")\n",
        "\n",
        "        ce = tl.cost.cross_entropy(y_prediction_batch_without_softmax, y_batch, name='cost')\n",
        "\n",
        "        \"\"\" 需给后面的全连接层引入L2 normalization，惩罚模型的复杂度，避免overfitting \"\"\"\n",
        "        # L2 for the MLP, without this, the accuracy will be reduced by 15%.\n",
        "        L2 = 0\n",
        "        for p in tl.layers.get_variables_with_name('relu/W', True, True):\n",
        "            L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n",
        "        # 加上L2模型复杂度惩罚项后，得到最终真正的cost\n",
        "        cost = ce + L2\n",
        "\n",
        "        correct_prediction = tf.equal(tf.cast(tf.argmax(y_prediction_batch_without_softmax, 1), y_TF_DTYPE), y_batch)\n",
        "        # correct_prediction = tf.Print(correct_prediction, [correct_prediction], \"correct_prediction: \")\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        return net, cost, accuracy, y_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jpISk8SQqCO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Train, validate and test"
      ]
    },
    {
      "metadata": {
        "id": "3iOAaIdbQoQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10050
        },
        "outputId": "75a5e659-629c-421f-a843-e2e572fdb056"
      },
      "cell_type": "code",
      "source": [
        "# Train, validate and test\n",
        "batch_size = 128\n",
        "n_epoch = 60\n",
        "n_step_per_epoch = int(len(train_set[y]) / batch_size)\n",
        "n_step = n_epoch * n_step_per_epoch\n",
        "print_freq = 1\n",
        "checkpoint_freq = 3\n",
        "learning_rate = 0.0001\n",
        "\n",
        "model_ckpt_file_name = os.path.join(working_directory, \"checkpoint\", \"model-quickdraw-cnn.ckpt\")\n",
        "resume = True  # load model, resume from previous checkpoint?\n",
        "\n",
        "\n",
        "def distort_fn(X, is_train=False):\n",
        "    # print('begin', X.shape, np.min(X), np.max(X))\n",
        "\n",
        "    if is_train == True:\n",
        "        # 1. Randomly flip the image horizontally.\n",
        "        X = tf.image.random_flip_left_right(X)\n",
        "\n",
        "    # X = tf.image.per_image_standardization(X)\n",
        "\n",
        "    # print('after norm', X.shape, np.min(X), np.max(X), np.mean(X))\n",
        "    return X\n",
        "\n",
        "\n",
        "def save_model():\n",
        "    model_type = \"saved-model\"\n",
        "    latest_model_directory = f'{model_type}-{time.strftime(\"%Y%m%d%H%M%S\", time.localtime())}'\n",
        "    saved_model_directory = os.path.join(working_directory, latest_model_directory)\n",
        "    if not os.path.exists(saved_model_directory):\n",
        "        tf.saved_model.simple_save(session, saved_model_directory,\n",
        "                                   inputs={\"X\": X_batch_ph},\n",
        "                                   outputs={\"y_output\": y_prediction_})\n",
        "        dist_directory = os.path.join(\".\", model_type)\n",
        "        if os.path.exists(dist_directory):\n",
        "            os.remove(dist_directory)\n",
        "        os.symlink(saved_model_directory, dist_directory, target_is_directory=True)\n",
        "\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "    session = tf.Session(config=config)\n",
        "\n",
        "    #\n",
        "    # Connect to tfdbg dashboard by ```http://localhost:6006#debugger```\n",
        "    # when the following command is issued.\n",
        "    #\n",
        "    # ```bash\n",
        "    # $ tensorboard --logdir logs --port 6006 --debugger_port 6064\n",
        "    # ```\n",
        "    #\n",
        "    # session = tfdebug.TensorBoardDebugWrapperSession(session, \"albert-mbp.local:6064\")\n",
        "\n",
        "    X_batch_ph = tf.placeholder(dtype=X_TF_DTYPE, shape=[None, image_height, image_width, image_depth], name='X_batch')\n",
        "    y_batch_ph = tf.placeholder(dtype=y_TF_DTYPE, shape=[None], name='y_batch')\n",
        "    # X_batch_ph = tf.placeholder(dtype=X_TF_DTYPE, shape=[batch_size, image_height, image_width, image_depth], name='X')\n",
        "    # y_batch_ph = tf.placeholder(dtype=y_TF_DTYPE, shape=[batch_size], name='y')\n",
        "\n",
        "    def perform_minibatch(run_list, X, y, batch_size, is_train=False):\n",
        "        n_batch, sum_loss, sum_accuracy = 0, 0, 0\n",
        "        for X_batch_a, y_batch_a in tl.iterate.minibatches(X, y, batch_size, shuffle=is_train):\n",
        "            # data augmentation for training\n",
        "            # X_batch_a = tl.prepro.threading_data(X_batch_a, fn=distort_fn, is_train=is_train)\n",
        "\n",
        "            cost, accuracy = 0, 0\n",
        "            if is_train:\n",
        "                _, cost, accuracy = session.run(\n",
        "                    run_list, feed_dict={X_batch_ph: X_batch_a, y_batch_ph: y_batch_a}\n",
        "                )\n",
        "            else:\n",
        "                cost, accuracy = session.run(\n",
        "                    run_list, feed_dict={X_batch_ph: X_batch_a, y_batch_ph: y_batch_a}\n",
        "                )\n",
        "\n",
        "            sum_loss += cost\n",
        "            sum_accuracy += accuracy\n",
        "            n_batch += 1\n",
        "        return n_batch, sum_loss, sum_accuracy\n",
        "\n",
        "\n",
        "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
        "        # Build the model\n",
        "        print(\"### Train Network model ###\")\n",
        "        network_, cost_, accuracy_, y_prediction_ = model_batch_normalization(\n",
        "            X_batch_ph, y_batch_ph, n_category, reuse=None, is_train=True\n",
        "        )\n",
        "        print(\"### Reuse this Train Network model for validation and test ###\")\n",
        "        _, cost_test_, accuracy_test_, y_prediction_test_ = model_batch_normalization(\n",
        "            X_batch_ph, y_batch_ph, n_category, reuse=True, is_train=False\n",
        "        )\n",
        "\n",
        "    # Define the training optimizer\n",
        "    with tf.device('/gpu:0'):  # <-- remove it if you don't have GPU\n",
        "        train_op_ = tf.train.AdamOptimizer(learning_rate).minimize(cost_)\n",
        "\n",
        "    tl.layers.initialize_global_variables(session)\n",
        "\n",
        "    # Attach the graph for TensorBoard writer\n",
        "    # tfboard_file_writer.add_graph(tf.get_default_graph())\n",
        "    tfboard_file_writer.add_graph(session.graph)\n",
        "\n",
        "    if resume and os.path.isfile(model_ckpt_file_name):\n",
        "        print(\"Load existing model \" + \"!\" * 10)\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(session, model_ckpt_file_name)\n",
        "\n",
        "    print(\"### Network parameters ###\")\n",
        "    network_.print_params(False)\n",
        "    print(\"### Network layers ###\")\n",
        "    network_.print_layers()\n",
        "\n",
        "    print('   learning_rate: %f' % learning_rate)\n",
        "    print('   batch_size: %d' % batch_size)\n",
        "    print('   n_epoch: %d, step in an epoch: %d, total n_step: %d' % (n_epoch, n_step_per_epoch, n_step))\n",
        "\n",
        "    step, sum_batch, sum_loss, sum_accuracy = 0, 0, 0, 0\n",
        "    for epoch in range(n_epoch):\n",
        "        start_time = time.time()\n",
        "\n",
        "        n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "            [train_op_, cost_, accuracy_],\n",
        "            train_set[X], train_set[y], batch_size, is_train=True\n",
        "        )\n",
        "        sum_batch += n_batch_a_epoch\n",
        "        sum_loss += cost_a_epoch\n",
        "        sum_accuracy += accuracy_a_epoch\n",
        "        step += n_batch_a_epoch\n",
        "\n",
        "        assert n_batch_a_epoch == n_step_per_epoch\n",
        "\n",
        "        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
        "            print(\"Epoch %d : Step %d-%d of %d took %fs\" %\n",
        "                  (epoch + 1, step - n_step_per_epoch, step, n_step, time.time() - start_time))\n",
        "            print(\"   train loss: %f\" % (sum_loss / sum_batch))\n",
        "            print(\"   train accuracy: %f\" % (sum_accuracy / sum_batch))\n",
        "            sum_batch, sum_loss, sum_accuracy = 0, 0, 0\n",
        "\n",
        "            n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "                [cost_test_, accuracy_test_],\n",
        "                validation_set[X], validation_set[y], batch_size\n",
        "            )\n",
        "            print(\"   validation loss: %f\" % (cost_a_epoch / n_batch_a_epoch))\n",
        "            print(\"   validation accuracy: %f\" % (accuracy_a_epoch / n_batch_a_epoch))\n",
        "\n",
        "            n_batch_a_epoch, cost_a_epoch, accuracy_a_epoch = perform_minibatch(\n",
        "                [cost_test_, accuracy_test_],\n",
        "                test_set[X], test_set[y], batch_size\n",
        "            )\n",
        "            print(\"   test loss: %f\" % (cost_a_epoch / n_batch_a_epoch))\n",
        "            print(\"   test accuracy: %f\" % (accuracy_a_epoch / n_batch_a_epoch))\n",
        "\n",
        "        # Save model when checkpoint\n",
        "        if (epoch + 1) % checkpoint_freq == 0:\n",
        "            print(\"Saving checkpoint... \" + \"!\" * 10)\n",
        "            saver = tf.train.Saver()\n",
        "            save_path = saver.save(session, model_ckpt_file_name)\n",
        "            print(\"Saving model... \" + \"!\" * 10)\n",
        "            save_model()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Train Network model ###\n",
            "[TL] InputLayer  model/input: (?, 28, 28, 1)\n",
            "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 1.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 3136\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (3136, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "### Reuse this Train Network model for validation and test ###\n",
            "[TL] InputLayer  model/input: (?, 28, 28, 1)\n",
            "[TL] Conv2d model/cnn1: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch1: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool1: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] Conv2d model/cnn2: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNormLayer model/batch2: decay: 0.000000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] MaxPool2d model/pool2: filter_size: (3, 3) strides: (2, 2) padding: SAME\n",
            "[TL] FlattenLayer model/flatten: 3136\n",
            "[TL] DenseLayer  model/d1relu: 384 relu\n",
            "[TL] DenseLayer  model/d2relu: 192 relu\n",
            "[TL] DenseLayer  model/output: 10 No Activation\n",
            "[TL]   [*] geting variables with relu/W\n",
            "[TL]   got   0: model/d1relu/W:0   (3136, 384)\n",
            "[TL]   got   1: model/d2relu/W:0   (384, 192)\n",
            "[TL] WARNING: Function: `tensorlayer.layers.utils.initialize_global_variables` (in file: /usr/local/lib/python3.6/dist-packages/tensorlayer/layers/utils.py) is deprecated and will be removed after 2018-09-30.\n",
            "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
            "\n",
            "### Network parameters ###\n",
            "[TL]   param   0: model/cnn1/kernel:0  (3, 3, 1, 64)      float32_ref\n",
            "[TL]   param   1: model/batch1/beta:0  (64,)              float32_ref\n",
            "[TL]   param   2: model/batch1/gamma:0 (64,)              float32_ref\n",
            "[TL]   param   3: model/batch1/moving_mean:0 (64,)              float32_ref\n",
            "[TL]   param   4: model/batch1/moving_variance:0 (64,)              float32_ref\n",
            "[TL]   param   5: model/cnn2/kernel:0  (3, 3, 64, 64)     float32_ref\n",
            "[TL]   param   6: model/batch2/beta:0  (64,)              float32_ref\n",
            "[TL]   param   7: model/batch2/gamma:0 (64,)              float32_ref\n",
            "[TL]   param   8: model/batch2/moving_mean:0 (64,)              float32_ref\n",
            "[TL]   param   9: model/batch2/moving_variance:0 (64,)              float32_ref\n",
            "[TL]   param  10: model/d1relu/W:0     (3136, 384)        float32_ref\n",
            "[TL]   param  11: model/d1relu/b:0     (384,)             float32_ref\n",
            "[TL]   param  12: model/d2relu/W:0     (384, 192)         float32_ref\n",
            "[TL]   param  13: model/d2relu/b:0     (192,)             float32_ref\n",
            "[TL]   param  14: model/output/W:0     (192, 10)          float32_ref\n",
            "[TL]   param  15: model/output/b:0     (10,)              float32_ref\n",
            "[TL]   num of params: 1318410\n",
            "### Network layers ###\n",
            "[TL]   layer   0: X_batch:0            (?, 28, 28, 1)     float32\n",
            "[TL]   layer   1: model/cnn1/Conv2D:0  (?, 28, 28, 64)    float32\n",
            "[TL]   layer   2: model/batch1/Relu:0  (?, 28, 28, 64)    float32\n",
            "[TL]   layer   3: model/pool1/MaxPool:0 (?, 14, 14, 64)    float32\n",
            "[TL]   layer   4: model/cnn2/Conv2D:0  (?, 14, 14, 64)    float32\n",
            "[TL]   layer   5: model/batch2/Relu:0  (?, 14, 14, 64)    float32\n",
            "[TL]   layer   6: model/pool2/MaxPool:0 (?, 7, 7, 64)      float32\n",
            "[TL]   layer   7: model/flatten:0      (?, 3136)          float32\n",
            "[TL]   layer   8: model/d1relu/Relu:0  (?, 384)           float32\n",
            "[TL]   layer   9: model/d2relu/Relu:0  (?, 192)           float32\n",
            "[TL]   layer  10: model/output/bias_add:0 (?, 10)            float32\n",
            "   learning_rate: 0.000100\n",
            "   batch_size: 128\n",
            "   n_epoch: 60, step in an epoch: 3906, total n_step: 234360\n",
            "Epoch 1 : Step 0-3906 of 234360 took 64.592385s\n",
            "   train loss: 1.158116\n",
            "   train accuracy: 0.835917\n",
            "   validation loss: 0.554823\n",
            "   validation accuracy: 0.899957\n",
            "   test loss: 0.555464\n",
            "   test accuracy: 0.900527\n",
            "Epoch 2 : Step 3906-7812 of 234360 took 63.947128s\n",
            "   train loss: 0.452596\n",
            "   train accuracy: 0.908500\n",
            "   validation loss: 0.389576\n",
            "   validation accuracy: 0.914315\n",
            "   test loss: 0.388399\n",
            "   test accuracy: 0.915227\n",
            "Epoch 3 : Step 7812-11718 of 234360 took 56.953911s\n",
            "   train loss: 0.351899\n",
            "   train accuracy: 0.920941\n",
            "   validation loss: 0.330103\n",
            "   validation accuracy: 0.923625\n",
            "   test loss: 0.328672\n",
            "   test accuracy: 0.923867\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Pass your op to the equivalent parameter main_op instead.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007041244/saved_model.pb\n",
            "Epoch 4 : Step 11718-15624 of 234360 took 63.012437s\n",
            "   train loss: 0.309442\n",
            "   train accuracy: 0.928039\n",
            "   validation loss: 0.298546\n",
            "   validation accuracy: 0.929589\n",
            "   test loss: 0.295120\n",
            "   test accuracy: 0.931216\n",
            "Epoch 5 : Step 15624-19530 of 234360 took 63.038584s\n",
            "   train loss: 0.284680\n",
            "   train accuracy: 0.932294\n",
            "   validation loss: 0.284438\n",
            "   validation accuracy: 0.932551\n",
            "   test loss: 0.281372\n",
            "   test accuracy: 0.933881\n",
            "Epoch 6 : Step 19530-23436 of 234360 took 63.162901s\n",
            "   train loss: 0.267571\n",
            "   train accuracy: 0.936014\n",
            "   validation loss: 0.275259\n",
            "   validation accuracy: 0.932278\n",
            "   test loss: 0.271606\n",
            "   test accuracy: 0.933250\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007041629/saved_model.pb\n",
            "Epoch 7 : Step 23436-27342 of 234360 took 62.817790s\n",
            "   train loss: 0.254825\n",
            "   train accuracy: 0.938342\n",
            "   validation loss: 0.261108\n",
            "   validation accuracy: 0.936639\n",
            "   test loss: 0.257529\n",
            "   test accuracy: 0.937290\n",
            "Epoch 8 : Step 27342-31248 of 234360 took 62.672805s\n",
            "   train loss: 0.244844\n",
            "   train accuracy: 0.940734\n",
            "   validation loss: 0.254309\n",
            "   validation accuracy: 0.937619\n",
            "   test loss: 0.249842\n",
            "   test accuracy: 0.939646\n",
            "Epoch 9 : Step 31248-35154 of 234360 took 62.709819s\n",
            "   train loss: 0.236540\n",
            "   train accuracy: 0.942416\n",
            "   validation loss: 0.243914\n",
            "   validation accuracy: 0.939719\n",
            "   test loss: 0.241670\n",
            "   test accuracy: 0.940642\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007042012/saved_model.pb\n",
            "Epoch 10 : Step 35154-39060 of 234360 took 62.468968s\n",
            "   train loss: 0.230075\n",
            "   train accuracy: 0.944058\n",
            "   validation loss: 0.241730\n",
            "   validation accuracy: 0.940650\n",
            "   test loss: 0.239479\n",
            "   test accuracy: 0.941049\n",
            "Epoch 11 : Step 39060-42966 of 234360 took 62.406298s\n",
            "   train loss: 0.224272\n",
            "   train accuracy: 0.945375\n",
            "   validation loss: 0.240863\n",
            "   validation accuracy: 0.940027\n",
            "   test loss: 0.239034\n",
            "   test accuracy: 0.940544\n",
            "Epoch 12 : Step 42966-46872 of 234360 took 62.348862s\n",
            "   train loss: 0.219536\n",
            "   train accuracy: 0.946519\n",
            "   validation loss: 0.238573\n",
            "   validation accuracy: 0.940671\n",
            "   test loss: 0.236403\n",
            "   test accuracy: 0.941469\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007042355/saved_model.pb\n",
            "Epoch 13 : Step 46872-50778 of 234360 took 62.315679s\n",
            "   train loss: 0.214785\n",
            "   train accuracy: 0.947449\n",
            "   validation loss: 0.231985\n",
            "   validation accuracy: 0.942407\n",
            "   test loss: 0.228749\n",
            "   test accuracy: 0.943657\n",
            "Epoch 14 : Step 50778-54684 of 234360 took 61.915146s\n",
            "   train loss: 0.211149\n",
            "   train accuracy: 0.948171\n",
            "   validation loss: 0.235857\n",
            "   validation accuracy: 0.941434\n",
            "   test loss: 0.232032\n",
            "   test accuracy: 0.942535\n",
            "Epoch 15 : Step 54684-58590 of 234360 took 62.211509s\n",
            "   train loss: 0.207794\n",
            "   train accuracy: 0.949285\n",
            "   validation loss: 0.230384\n",
            "   validation accuracy: 0.942673\n",
            "   test loss: 0.228359\n",
            "   test accuracy: 0.942802\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007042737/saved_model.pb\n",
            "Epoch 16 : Step 58590-62496 of 234360 took 57.062423s\n",
            "   train loss: 0.204381\n",
            "   train accuracy: 0.950129\n",
            "   validation loss: 0.226108\n",
            "   validation accuracy: 0.943863\n",
            "   test loss: 0.225423\n",
            "   test accuracy: 0.944401\n",
            "Epoch 17 : Step 62496-66402 of 234360 took 56.999600s\n",
            "   train loss: 0.201672\n",
            "   train accuracy: 0.951043\n",
            "   validation loss: 0.227506\n",
            "   validation accuracy: 0.943541\n",
            "   test loss: 0.224210\n",
            "   test accuracy: 0.943854\n",
            "Epoch 18 : Step 66402-70308 of 234360 took 57.007776s\n",
            "   train loss: 0.199202\n",
            "   train accuracy: 0.951621\n",
            "   validation loss: 0.227295\n",
            "   validation accuracy: 0.942673\n",
            "   test loss: 0.226874\n",
            "   test accuracy: 0.943251\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007043104/saved_model.pb\n",
            "Epoch 19 : Step 70308-74214 of 234360 took 61.619951s\n",
            "   train loss: 0.197111\n",
            "   train accuracy: 0.952021\n",
            "   validation loss: 0.223136\n",
            "   validation accuracy: 0.944423\n",
            "   test loss: 0.220641\n",
            "   test accuracy: 0.945383\n",
            "Epoch 20 : Step 74214-78120 of 234360 took 61.238165s\n",
            "   train loss: 0.194904\n",
            "   train accuracy: 0.952929\n",
            "   validation loss: 0.225992\n",
            "   validation accuracy: 0.943933\n",
            "   test loss: 0.224310\n",
            "   test accuracy: 0.943952\n",
            "Epoch 21 : Step 78120-82026 of 234360 took 61.460241s\n",
            "   train loss: 0.192543\n",
            "   train accuracy: 0.953581\n",
            "   validation loss: 0.221674\n",
            "   validation accuracy: 0.945334\n",
            "   test loss: 0.219715\n",
            "   test accuracy: 0.945593\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007043443/saved_model.pb\n",
            "Epoch 22 : Step 82026-85932 of 234360 took 60.433082s\n",
            "   train loss: 0.190200\n",
            "   train accuracy: 0.954229\n",
            "   validation loss: 0.222321\n",
            "   validation accuracy: 0.944549\n",
            "   test loss: 0.220522\n",
            "   test accuracy: 0.945172\n",
            "Epoch 23 : Step 85932-89838 of 234360 took 60.330016s\n",
            "   train loss: 0.188541\n",
            "   train accuracy: 0.954451\n",
            "   validation loss: 0.223539\n",
            "   validation accuracy: 0.944122\n",
            "   test loss: 0.222236\n",
            "   test accuracy: 0.944822\n",
            "Epoch 24 : Step 89838-93744 of 234360 took 60.673554s\n",
            "   train loss: 0.186490\n",
            "   train accuracy: 0.955059\n",
            "   validation loss: 0.218056\n",
            "   validation accuracy: 0.945789\n",
            "   test loss: 0.215410\n",
            "   test accuracy: 0.946533\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007043820/saved_model.pb\n",
            "Epoch 25 : Step 93744-97650 of 234360 took 60.928179s\n",
            "   train loss: 0.185218\n",
            "   train accuracy: 0.955349\n",
            "   validation loss: 0.219840\n",
            "   validation accuracy: 0.944794\n",
            "   test loss: 0.217084\n",
            "   test accuracy: 0.946042\n",
            "Epoch 26 : Step 97650-101556 of 234360 took 60.689742s\n",
            "   train loss: 0.183313\n",
            "   train accuracy: 0.956177\n",
            "   validation loss: 0.220785\n",
            "   validation accuracy: 0.945312\n",
            "   test loss: 0.220950\n",
            "   test accuracy: 0.945495\n",
            "Epoch 27 : Step 101556-105462 of 234360 took 60.606998s\n",
            "   train loss: 0.181974\n",
            "   train accuracy: 0.956333\n",
            "   validation loss: 0.223736\n",
            "   validation accuracy: 0.944290\n",
            "   test loss: 0.222273\n",
            "   test accuracy: 0.944387\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007044158/saved_model.pb\n",
            "Epoch 28 : Step 105462-109368 of 234360 took 60.234536s\n",
            "   train loss: 0.180364\n",
            "   train accuracy: 0.956829\n",
            "   validation loss: 0.225222\n",
            "   validation accuracy: 0.944101\n",
            "   test loss: 0.223169\n",
            "   test accuracy: 0.944331\n",
            "Epoch 29 : Step 109368-113274 of 234360 took 60.330982s\n",
            "   train loss: 0.179027\n",
            "   train accuracy: 0.957213\n",
            "   validation loss: 0.226077\n",
            "   validation accuracy: 0.943821\n",
            "   test loss: 0.224761\n",
            "   test accuracy: 0.943742\n",
            "Epoch 30 : Step 113274-117180 of 234360 took 60.332395s\n",
            "   train loss: 0.177403\n",
            "   train accuracy: 0.957785\n",
            "   validation loss: 0.221736\n",
            "   validation accuracy: 0.945123\n",
            "   test loss: 0.219394\n",
            "   test accuracy: 0.946280\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007044535/saved_model.pb\n",
            "Epoch 31 : Step 117180-121086 of 234360 took 60.125053s\n",
            "   train loss: 0.176016\n",
            "   train accuracy: 0.958407\n",
            "   validation loss: 0.221253\n",
            "   validation accuracy: 0.945144\n",
            "   test loss: 0.221228\n",
            "   test accuracy: 0.945509\n",
            "Epoch 32 : Step 121086-124992 of 234360 took 60.019822s\n",
            "   train loss: 0.174642\n",
            "   train accuracy: 0.958939\n",
            "   validation loss: 0.222518\n",
            "   validation accuracy: 0.945088\n",
            "   test loss: 0.222262\n",
            "   test accuracy: 0.944541\n",
            "Epoch 33 : Step 124992-128898 of 234360 took 59.706829s\n",
            "   train loss: 0.173928\n",
            "   train accuracy: 0.958859\n",
            "   validation loss: 0.222300\n",
            "   validation accuracy: 0.946293\n",
            "   test loss: 0.223463\n",
            "   test accuracy: 0.945186\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007044910/saved_model.pb\n",
            "Epoch 34 : Step 128898-132804 of 234360 took 60.332853s\n",
            "   train loss: 0.172422\n",
            "   train accuracy: 0.959471\n",
            "   validation loss: 0.226061\n",
            "   validation accuracy: 0.944395\n",
            "   test loss: 0.224125\n",
            "   test accuracy: 0.944948\n",
            "Epoch 35 : Step 132804-136710 of 234360 took 60.223099s\n",
            "   train loss: 0.171500\n",
            "   train accuracy: 0.959719\n",
            "   validation loss: 0.219902\n",
            "   validation accuracy: 0.946160\n",
            "   test loss: 0.219982\n",
            "   test accuracy: 0.945860\n",
            "Epoch 36 : Step 136710-140616 of 234360 took 59.784693s\n",
            "   train loss: 0.170090\n",
            "   train accuracy: 0.960045\n",
            "   validation loss: 0.223118\n",
            "   validation accuracy: 0.945509\n",
            "   test loss: 0.222671\n",
            "   test accuracy: 0.944653\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007045246/saved_model.pb\n",
            "Epoch 37 : Step 140616-144522 of 234360 took 59.399109s\n",
            "   train loss: 0.169181\n",
            "   train accuracy: 0.960463\n",
            "   validation loss: 0.223538\n",
            "   validation accuracy: 0.945418\n",
            "   test loss: 0.222736\n",
            "   test accuracy: 0.945916\n",
            "Epoch 38 : Step 144522-148428 of 234360 took 59.834995s\n",
            "   train loss: 0.167621\n",
            "   train accuracy: 0.961154\n",
            "   validation loss: 0.224317\n",
            "   validation accuracy: 0.945614\n",
            "   test loss: 0.223981\n",
            "   test accuracy: 0.945551\n",
            "Epoch 39 : Step 148428-152334 of 234360 took 59.585822s\n",
            "   train loss: 0.166603\n",
            "   train accuracy: 0.961638\n",
            "   validation loss: 0.223765\n",
            "   validation accuracy: 0.945032\n",
            "   test loss: 0.225723\n",
            "   test accuracy: 0.944667\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007045621/saved_model.pb\n",
            "Epoch 40 : Step 152334-156240 of 234360 took 59.164258s\n",
            "   train loss: 0.165819\n",
            "   train accuracy: 0.961824\n",
            "   validation loss: 0.225979\n",
            "   validation accuracy: 0.945235\n",
            "   test loss: 0.225379\n",
            "   test accuracy: 0.945523\n",
            "Epoch 41 : Step 156240-160146 of 234360 took 59.382235s\n",
            "   train loss: 0.164724\n",
            "   train accuracy: 0.962172\n",
            "   validation loss: 0.226839\n",
            "   validation accuracy: 0.945137\n",
            "   test loss: 0.225045\n",
            "   test accuracy: 0.944906\n",
            "Epoch 42 : Step 160146-164052 of 234360 took 59.151559s\n",
            "   train loss: 0.163750\n",
            "   train accuracy: 0.962254\n",
            "   validation loss: 0.229333\n",
            "   validation accuracy: 0.943891\n",
            "   test loss: 0.228605\n",
            "   test accuracy: 0.944064\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007045954/saved_model.pb\n",
            "Epoch 43 : Step 164052-167958 of 234360 took 58.742006s\n",
            "   train loss: 0.162791\n",
            "   train accuracy: 0.962852\n",
            "   validation loss: 0.232074\n",
            "   validation accuracy: 0.944269\n",
            "   test loss: 0.230493\n",
            "   test accuracy: 0.944022\n",
            "Epoch 44 : Step 167958-171864 of 234360 took 58.750481s\n",
            "   train loss: 0.161956\n",
            "   train accuracy: 0.963078\n",
            "   validation loss: 0.228215\n",
            "   validation accuracy: 0.945453\n",
            "   test loss: 0.226890\n",
            "   test accuracy: 0.945747\n",
            "Epoch 45 : Step 171864-175770 of 234360 took 59.015908s\n",
            "   train loss: 0.161472\n",
            "   train accuracy: 0.963272\n",
            "   validation loss: 0.229048\n",
            "   validation accuracy: 0.945242\n",
            "   test loss: 0.229516\n",
            "   test accuracy: 0.945242\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007050326/saved_model.pb\n",
            "Epoch 46 : Step 175770-179676 of 234360 took 58.533766s\n",
            "   train loss: 0.160767\n",
            "   train accuracy: 0.963432\n",
            "   validation loss: 0.229950\n",
            "   validation accuracy: 0.945348\n",
            "   test loss: 0.229965\n",
            "   test accuracy: 0.945270\n",
            "Epoch 47 : Step 179676-183582 of 234360 took 58.624089s\n",
            "   train loss: 0.159322\n",
            "   train accuracy: 0.964050\n",
            "   validation loss: 0.229797\n",
            "   validation accuracy: 0.945761\n",
            "   test loss: 0.228521\n",
            "   test accuracy: 0.944794\n",
            "Epoch 48 : Step 183582-187488 of 234360 took 58.595352s\n",
            "   train loss: 0.158981\n",
            "   train accuracy: 0.964086\n",
            "   validation loss: 0.229514\n",
            "   validation accuracy: 0.945670\n",
            "   test loss: 0.228670\n",
            "   test accuracy: 0.945453\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007050658/saved_model.pb\n",
            "Epoch 49 : Step 187488-191394 of 234360 took 58.558575s\n",
            "   train loss: 0.157801\n",
            "   train accuracy: 0.964390\n",
            "   validation loss: 0.235319\n",
            "   validation accuracy: 0.943639\n",
            "   test loss: 0.230795\n",
            "   test accuracy: 0.944443\n",
            "Epoch 50 : Step 191394-195300 of 234360 took 58.345025s\n",
            "   train loss: 0.157392\n",
            "   train accuracy: 0.964818\n",
            "   validation loss: 0.235780\n",
            "   validation accuracy: 0.944220\n",
            "   test loss: 0.233807\n",
            "   test accuracy: 0.944247\n",
            "Epoch 51 : Step 195300-199206 of 234360 took 58.474812s\n",
            "   train loss: 0.156291\n",
            "   train accuracy: 0.965080\n",
            "   validation loss: 0.236131\n",
            "   validation accuracy: 0.944269\n",
            "   test loss: 0.235042\n",
            "   test accuracy: 0.944443\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007051029/saved_model.pb\n",
            "Epoch 52 : Step 199206-203112 of 234360 took 58.208148s\n",
            "   train loss: 0.155894\n",
            "   train accuracy: 0.965280\n",
            "   validation loss: 0.235186\n",
            "   validation accuracy: 0.943898\n",
            "   test loss: 0.231827\n",
            "   test accuracy: 0.945663\n",
            "Epoch 53 : Step 203112-207018 of 234360 took 58.213047s\n",
            "   train loss: 0.154684\n",
            "   train accuracy: 0.965736\n",
            "   validation loss: 0.233440\n",
            "   validation accuracy: 0.944829\n",
            "   test loss: 0.231514\n",
            "   test accuracy: 0.945298\n",
            "Epoch 54 : Step 207018-210924 of 234360 took 58.312944s\n",
            "   train loss: 0.154603\n",
            "   train accuracy: 0.965930\n",
            "   validation loss: 0.237542\n",
            "   validation accuracy: 0.944276\n",
            "   test loss: 0.237966\n",
            "   test accuracy: 0.943195\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007051400/saved_model.pb\n",
            "Epoch 55 : Step 210924-214830 of 234360 took 58.237864s\n",
            "   train loss: 0.153341\n",
            "   train accuracy: 0.965938\n",
            "   validation loss: 0.238456\n",
            "   validation accuracy: 0.945327\n",
            "   test loss: 0.235419\n",
            "   test accuracy: 0.945411\n",
            "Epoch 56 : Step 214830-218736 of 234360 took 58.309646s\n",
            "   train loss: 0.152731\n",
            "   train accuracy: 0.966460\n",
            "   validation loss: 0.239182\n",
            "   validation accuracy: 0.943891\n",
            "   test loss: 0.235394\n",
            "   test accuracy: 0.944261\n",
            "Epoch 57 : Step 218736-222642 of 234360 took 58.183021s\n",
            "   train loss: 0.152116\n",
            "   train accuracy: 0.966664\n",
            "   validation loss: 0.244471\n",
            "   validation accuracy: 0.943667\n",
            "   test loss: 0.241908\n",
            "   test accuracy: 0.943756\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007051731/saved_model.pb\n",
            "Epoch 58 : Step 222642-226548 of 234360 took 58.349438s\n",
            "   train loss: 0.151115\n",
            "   train accuracy: 0.966988\n",
            "   validation loss: 0.239250\n",
            "   validation accuracy: 0.944136\n",
            "   test loss: 0.237503\n",
            "   test accuracy: 0.944092\n",
            "Epoch 59 : Step 226548-230454 of 234360 took 58.128834s\n",
            "   train loss: 0.150499\n",
            "   train accuracy: 0.967292\n",
            "   validation loss: 0.245370\n",
            "   validation accuracy: 0.943849\n",
            "   test loss: 0.242619\n",
            "   test accuracy: 0.943363\n",
            "Epoch 60 : Step 230454-234360 of 234360 took 58.241380s\n",
            "   train loss: 0.149762\n",
            "   train accuracy: 0.967528\n",
            "   validation loss: 0.243740\n",
            "   validation accuracy: 0.944325\n",
            "   test loss: 0.241388\n",
            "   test accuracy: 0.943517\n",
            "Saving checkpoint... !!!!!!!!!!\n",
            "Saving model... !!!!!!!!!!\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007052102/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UbFcV5edSMlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save the trained model"
      ]
    },
    {
      "metadata": {
        "id": "4wFMK2v6SMvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "094e85a4-3958-4866-aed2-5517c1180419"
      },
      "cell_type": "code",
      "source": [
        "save_model()\n",
        "\n",
        "tfboard_file_writer.flush()\n",
        "tfboard_file_writer.close()\n",
        "\n",
        "session.close()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: data/saved-model-20181007052149/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3e3lOIaaTd0s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert to TensorFlow.js web model"
      ]
    },
    {
      "metadata": {
        "id": "CrdV5utoTeB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1040
        },
        "outputId": "92702ab0-e96e-4078-ba5a-b75e161a851a"
      },
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=tf_saved_model --output_node_names=\"model/y_output\" saved-model web-model\n",
        "!echo \"Current directory ->\"\n",
        "!ls -la\n",
        "!echo \"web-model directory ->\"\n",
        "!ls -la web-model\n",
        "!echo \"saved-model directory ->\"\n",
        "!ls -la saved-model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2018-10-07 05:22:03.050093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2018-10-07 05:22:03.050558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 10.69GiB\n",
            "2018-10-07 05:22:03.050599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
            "2018-10-07 05:22:03.480260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-10-07 05:22:03.480328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
            "2018-10-07 05:22:03.480366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
            "2018-10-07 05:22:03.480641: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2018-10-07 05:22:03.480704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10357 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2018-10-07 05:22:04.473145: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: graph_to_optimize\n",
            "2018-10-07 05:22:04.473203: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   debug_stripper: Graph size after: 66 nodes (0), 67 edges (0), time = 0.063ms.\n",
            "2018-10-07 05:22:04.473230: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   model_pruner: Graph size after: 50 nodes (-16), 51 edges (-16), time = 0.343ms.\n",
            "2018-10-07 05:22:04.473251: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 30 nodes (-20), 29 edges (-22), time = 14.366ms.\n",
            "2018-10-07 05:22:04.473271: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   arithmetic_optimizer: Graph size after: 29 nodes (-1), 29 edges (0), time = 15.613ms.\n",
            "2018-10-07 05:22:04.473290: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   dependency_optimizer: Graph size after: 26 nodes (-3), 25 edges (-4), time = 0.287ms.\n",
            "2018-10-07 05:22:04.473310: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   model_pruner: Graph size after: 26 nodes (0), 25 edges (0), time = 0.105ms.\n",
            "2018-10-07 05:22:04.473329: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 26 nodes (0), 25 edges (0), time = 3.344ms.\n",
            "2018-10-07 05:22:04.473348: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   arithmetic_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 5.844ms.\n",
            "2018-10-07 05:22:04.473389: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   dependency_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.239ms.\n",
            "2018-10-07 05:22:04.473411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   debug_stripper: Graph size after: 26 nodes (0), 25 edges (0), time = 0.049ms.\n",
            "2018-10-07 05:22:04.473432: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   model_pruner: Graph size after: 26 nodes (0), 25 edges (0), time = 0.083ms.\n",
            "2018-10-07 05:22:04.473451: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 26 nodes (0), 25 edges (0), time = 3.45ms.\n",
            "2018-10-07 05:22:04.473470: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   arithmetic_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 5.705ms.\n",
            "2018-10-07 05:22:04.473488: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   dependency_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.306ms.\n",
            "2018-10-07 05:22:04.473507: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   model_pruner: Graph size after: 26 nodes (0), 25 edges (0), time = 0.121ms.\n",
            "2018-10-07 05:22:04.473526: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 26 nodes (0), 25 edges (0), time = 3.18ms.\n",
            "2018-10-07 05:22:04.473545: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   arithmetic_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 5.824ms.\n",
            "2018-10-07 05:22:04.473563: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   dependency_optimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.256ms.\n",
            "Writing weight file web-model/tensorflowjs_model.pb...\n",
            "2018-10-07 05:22:04.488767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
            "2018-10-07 05:22:04.488876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-10-07 05:22:04.488904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
            "2018-10-07 05:22:04.488926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
            "2018-10-07 05:22:04.489234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10357 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Current directory ->\n",
            "total 16496\n",
            "drwxr-xr-x  1 root root     4096 Oct  7 05:21 .\n",
            "drwxr-xr-x  1 root root     4096 Oct  7 01:03 ..\n",
            "drwxr-xr-x  4 root root     4096 Sep 28 23:11 .config\n",
            "drwxr-xr-x 83 root root     4096 Oct  7 05:21 data\n",
            "drwxr-xr-x  2 root root     4096 Oct  7 04:09 logs\n",
            "drwxr-xr-x  2 root root     4096 Sep 28 23:32 sample_data\n",
            "lrwxrwxrwx  1 root root       31 Oct  7 05:21 saved-model -> data/saved-model-20181007052149\n",
            "-rw-r--r--  1 root root 11780122 Oct  7 03:53 saved-model.zip\n",
            "drwxr-xr-x  2 root root     4096 Oct  7 05:22 web-model\n",
            "-rw-r--r--  1 root root  5078227 Oct  7 03:53 web-model.zip\n",
            "web-model directory ->\n",
            "total 5316\n",
            "drwxr-xr-x 2 root root    4096 Oct  7 05:22 .\n",
            "drwxr-xr-x 1 root root    4096 Oct  7 05:21 ..\n",
            "-rw-r--r-- 1 root root 4194304 Oct  7 05:22 group1-shard1of2\n",
            "-rw-r--r-- 1 root root 1077296 Oct  7 05:22 group1-shard2of2\n",
            "-rw-r--r-- 1 root root  152213 Oct  7 05:22 tensorflowjs_model.pb\n",
            "-rw-r--r-- 1 root root     706 Oct  7 05:22 weights_manifest.json\n",
            "saved-model directory ->\n",
            "lrwxrwxrwx 1 root root 31 Oct  7 05:21 saved-model -> data/saved-model-20181007052149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q8g244v4VZUj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Zip and download the models"
      ]
    },
    {
      "metadata": {
        "id": "0fzcYLP3VB97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "16100588-cf2f-4fff-d872-63013b8f4eea"
      },
      "cell_type": "code",
      "source": [
        "!zip -r web-model.zip web-model\n",
        "!zip -r saved-model.zip saved-model\n",
        "!ls -la *.zip\n",
        "\n",
        "from google.colab import files\n",
        "files.download('web-model.zip')\n",
        "files.download('saved-model.zip')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: web-model/ (stored 0%)\n",
            "updating: web-model/weights_manifest.json (deflated 71%)\n",
            "updating: web-model/tensorflowjs_model.pb (deflated 8%)\n",
            "updating: web-model/group1-shard2of2 (deflated 7%)\n",
            "updating: web-model/group1-shard1of2 (deflated 6%)\n",
            "updating: saved-model/ (stored 0%)\n",
            "updating: saved-model/variables/ (stored 0%)\n",
            "updating: saved-model/variables/variables.data-00000-of-00001 (deflated 27%)\n",
            "updating: saved-model/variables/variables.index (deflated 47%)\n",
            "updating: saved-model/saved_model.pb (deflated 93%)\n",
            "-rw-r--r-- 1 root root 11650483 Oct  7 05:22 saved-model.zip\n",
            "-rw-r--r-- 1 root root  5071314 Oct  7 05:22 web-model.zip\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}